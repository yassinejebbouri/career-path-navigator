{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Set\n",
    "\n",
    "# ── tweakables ───────────────────────────────────────────────────────────────\n",
    "NODES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/nodes_final.csv\")\n",
    "EDGES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/relationships_final.csv\")\n",
    "\n",
    "BETA        = 8.0      # spreads “good” (≈0) vs “bad” (≪0) scores\n",
    "LAMBDA      = 1.0      # 1.0 ⇒ cost = structural only (no embeddings)\n",
    "\n",
    "# If you *do* want to include semantic smoothness later:\n",
    "EMB_PATH    = None     # e.g. Path(\"node_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ nodes_df columns: ['nodeId:ID', 'name', 'definition', 'labels:LABEL']\n",
      "→ edges_df columns: [':START_ID', ':END_ID', 'type', 'score:float', 'predicted:boolean']\n",
      "✅ Graph loaded!\n",
      "   Nodes: 1148, Edges: 4380\n",
      "   Example edge 0→8 has attrs {'score': -0.8655019998550415, 'predicted': False}\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 ── Load graph with dynamic column detection + debug ────────────────\n",
    "def load_graph(nodes_csv: Path, edges_csv: Path) -> Tuple[nx.DiGraph, Dict[int, Dict]]:\n",
    "    # 1. Read CSVs\n",
    "    nodes_df = pd.read_csv(nodes_csv)\n",
    "    edges_df = pd.read_csv(edges_csv)\n",
    "    \n",
    "    # 2. Debug: show what columns we actually have\n",
    "    print(\"→ nodes_df columns:\", nodes_df.columns.tolist())\n",
    "    print(\"→ edges_df columns:\", edges_df.columns.tolist())\n",
    "    \n",
    "    # 3. Dynamically pick the right column names\n",
    "    nid_col       = next(c for c in nodes_df.columns if \"ID\"   in c)\n",
    "    name_col      = next(c for c in nodes_df.columns if \"name\" in c.lower())\n",
    "    def_col       = next(c for c in nodes_df.columns if \"def\"  in c.lower())\n",
    "    labels_col    = next(c for c in nodes_df.columns if \"label\" in c.lower())\n",
    "    \n",
    "    src_col       = next(c for c in edges_df.columns if \"start\" in c.lower())\n",
    "    dst_col       = next(c for c in edges_df.columns if \"end\"   in c.lower())\n",
    "    score_col     = next(c for c in edges_df.columns if \"score\" in c.lower())\n",
    "    predicted_col = next(c for c in edges_df.columns if \"predicted\" in c.lower())\n",
    "    \n",
    "    # 4. Build the graph\n",
    "    G = nx.DiGraph()\n",
    "    node_props = {}\n",
    "    \n",
    "    for _, row in nodes_df.iterrows():\n",
    "        nid = int(row[nid_col])\n",
    "        props = {\n",
    "            \"name\":       row[name_col],\n",
    "            \"definition\": row[def_col],\n",
    "            \"labels\":     row[labels_col].split(\";\")\n",
    "        }\n",
    "        G.add_node(nid, **props)\n",
    "        node_props[nid] = props\n",
    "    \n",
    "    for _, row in edges_df.iterrows():\n",
    "        src   = int(row[src_col])\n",
    "        dst   = int(row[dst_col])\n",
    "        score = float(row[score_col])\n",
    "        pred  = bool(row[predicted_col])\n",
    "        G.add_edge(src, dst, score=score, predicted=pred)\n",
    "    \n",
    "    return G, node_props\n",
    "\n",
    "# Run it and debug\n",
    "G, props = load_graph(NODES_CSV, EDGES_CSV)\n",
    "print(\"✅ Graph loaded!\")\n",
    "print(f\"   Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "# show one example\n",
    "u, v, d = next(iter(G.edges(data=True)))\n",
    "print(f\"   Example edge {u}→{v} has attrs {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oriented DAG: 1,143 nodes, 4,380 edges\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx, math, torch, numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# ---------- 1) orient every edge (prereq → dependent) ----------\n",
    "H = nx.DiGraph()\n",
    "for u, v, d in G.edges(data=True):\n",
    "    # if reverse edge exists, keep only the prerequisite direction\n",
    "    if G.has_edge(v, u) and abs(G[v][u]['score']) < abs(d['score']):\n",
    "        continue                    # v→u is the stronger prereq arrow\n",
    "    H.add_edge(u, v, score=d['score'])\n",
    "\n",
    "print(f\"Oriented DAG: {H.number_of_nodes():,} nodes, {H.number_of_edges():,} edges\")\n",
    "\n",
    "# ---------- 2) longest-path layer number ----------\n",
    "layer = {}\n",
    "for root in [n for n in H if H.in_degree(n)==0]:\n",
    "    for n, depth in nx.single_source_shortest_path_length(H, root).items():\n",
    "        layer[n] = max(layer.get(n, 0), depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1148 node embeddings\n",
      "Example – node 0 → embedding shape (128,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# if you saved with torch.save(node_embeddings, \"node_embeddings.pt\")\n",
    "node_embeddings = torch.load(\"node_embeddings.pt\", map_location=\"cpu\")\n",
    "\n",
    "# quick sanity check\n",
    "print(f\"Loaded {len(node_embeddings)} node embeddings\")\n",
    "nid, emb = next(iter(node_embeddings.items()))\n",
    "print(f\"Example – node {nid} → embedding shape {tuple(emb.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEM_WEIGHT   = 0.3     # semantic share (0 → ignore embeddings)\n",
    "SKIP_FACTOR  = 0.3     # penalty per extra layer jump beyond +1\n",
    "\n",
    "# pre-compute max |score|\n",
    "max_abs = max(abs(d['score']) for _,_,d in H.edges(data=True))\n",
    "\n",
    "def cosine(u, v):\n",
    "    return float((u @ v) / (norm(u)*norm(v) + 1e-9))\n",
    "\n",
    "for u, v, d in H.edges(data=True):\n",
    "    struct = abs(d['score']) / max_abs\n",
    "\n",
    "    # safe semantic term\n",
    "    if SEM_WEIGHT and (u in node_embeddings) and (v in node_embeddings):\n",
    "        sem = 1 - cosine(node_embeddings[u], node_embeddings[v])\n",
    "    else:\n",
    "        sem = 0.0\n",
    "\n",
    "    skip = max(layer[v] - layer[u] - 1, 0) * SKIP_FACTOR\n",
    "\n",
    "    d['cost'] = (1-SEM_WEIGHT)*struct + SEM_WEIGHT*sem + skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  skipped 1143 nodes with no name/title: [0, 8, 9, 43, 63, 76, 1, 2, 4, 14]…\n"
     ]
    }
   ],
   "source": [
    "def get_label(d):\n",
    "    return d.get(\"name\") or d.get(\"title\")\n",
    "\n",
    "# Build name2id, skipping label-less nodes\n",
    "name2id = {}\n",
    "missing_labels = []\n",
    "for nid, d in H.nodes(data=True):\n",
    "    lbl = get_label(d)\n",
    "    if lbl is None:\n",
    "        missing_labels.append(nid)\n",
    "    else:\n",
    "        name2id[lbl.lower()] = nid\n",
    "\n",
    "if missing_labels:\n",
    "    print(f\"⚠️  skipped {len(missing_labels)} nodes with no name/title: {missing_labels[:10]}…\")\n",
    "\n",
    "# Build id2name similarly\n",
    "id2name = {nid:lbl for lbl,nid in ((get_label(d), nid) for nid,d in H.nodes(data=True)) if lbl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MACHINE LEARNING ENGINEER requires 30 skills ===\n",
      "\n",
      "• algorithms                      steps=2 cost=0.013\n",
      "     genai manager → genai → algorithms\n",
      "\n",
      "• aws                             steps=2 cost=0.045\n",
      "     machine learning developer → sagemaker → aws\n",
      "\n",
      "• c++                             steps=2 cost=0.398\n",
      "     llm intern → address locator → c++\n",
      "\n",
      "• collaboration                   steps=1 cost=0.000\n",
      "     ai architect → collaboration\n",
      "\n",
      "• communications                  steps=1 cost=0.000\n",
      "     ai architect → communications\n",
      "\n",
      "• computer science                steps=6 cost=0.113\n",
      "     computer animation → rendering (computer graphics) → computer graphics → image representation → feature extraction → algorithm → computer science\n",
      "\n",
      "• creativity                      steps=1 cost=0.000\n",
      "     ai research researcher → creativity\n",
      "\n",
      "• data science                    steps=2 cost=0.033\n",
      "     machine learning engineer → docker → data science\n",
      "\n",
      "• deep learning                   steps=2 cost=0.146\n",
      "     llm developer → llms → deep learning\n",
      "\n",
      "• docker                          steps=1 cost=0.000\n",
      "     machine learning engineer → docker\n",
      "\n",
      "• gcp                             steps=2 cost=0.049\n",
      "     ocr engineer → google cloud platform → gcp\n",
      "\n",
      "• google cloud platform (gcp)     steps=2 cost=0.041\n",
      "     ocr engineer → google cloud platform → google cloud platform (gcp)\n",
      "\n",
      "• infrastructure                  steps=1 cost=0.000\n",
      "     ai research researcher → infrastructure\n",
      "\n",
      "• innovation                      steps=1 cost=0.000\n",
      "     ai architect → innovation\n",
      "\n",
      "• integration                     steps=1 cost=0.000\n",
      "     ai architect → integration\n",
      "\n",
      "• java                            steps=1 cost=0.000\n",
      "     ai research researcher → java\n",
      "\n",
      "• kubernetes                      steps=2 cost=0.033\n",
      "     nlp engineer → elasticsearch → kubernetes\n",
      "\n",
      "• library                         steps=1 cost=0.000\n",
      "     deep learning engineer → library\n",
      "\n",
      "• machine learning                steps=3 cost=0.179\n",
      "     reinforcement learning engineer → robotics → speech recognition → machine learning\n",
      "\n",
      "• machine learning methods        steps=6 cost=0.832\n",
      "     computer animation → rendering (computer graphics) → computer graphics → image representation → pattern recognition or machine learning → ensemble classifier → machine learning methods\n",
      "\n",
      "• management                      steps=1 cost=0.000\n",
      "     ai architect → management\n",
      "\n",
      "• operations                      steps=1 cost=0.000\n",
      "     ai architect → operations\n",
      "\n",
      "• planning                        steps=1 cost=0.000\n",
      "     ai architect → planning\n",
      "\n",
      "• python                          steps=1 cost=0.000\n",
      "     ai architect → python\n",
      "\n",
      "• pytorch                         steps=1 cost=0.000\n",
      "     ai engineer → pytorch\n",
      "\n",
      "• research                        steps=1 cost=0.000\n",
      "     ai engineer → research\n",
      "\n",
      "• scalability                     steps=1 cost=0.000\n",
      "     ai architect → scalability\n",
      "\n",
      "• scikit-learn                    steps=2 cost=0.032\n",
      "     genai manager → rpa → scikit-learn\n",
      "\n",
      "• software engineering            steps=2 cost=0.102\n",
      "     ai architect → software development → software engineering\n",
      "\n",
      "• tensorflow                      steps=2 cost=0.095\n",
      "     speech recognition scientist → fairseq → tensorflow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from heapq import heappush, heappop\n",
    "\n",
    "# ─── 1. Build label maps from props, not H ─────────────────────────────────\n",
    "def get_label(d):\n",
    "    # for skills/concepts: \"name\"; for jobs: \"title\"\n",
    "    return d.get(\"name\") or d.get(\"title\")\n",
    "\n",
    "name2id = {}\n",
    "for nid, d in props.items():\n",
    "    lbl = get_label(d)\n",
    "    if lbl:\n",
    "        name2id[lbl.lower()] = nid\n",
    "    else:\n",
    "        print(f\"⚠️  Node {nid!r} has no name or title in props\")\n",
    "\n",
    "id2name = {nid: get_label(d) for nid, d in props.items() if get_label(d)}\n",
    "\n",
    "# ─── 2. Dijkstra to find best cost path to one skill ────────────────────────\n",
    "def best_path_to_skill(skill_id):\n",
    "    roots = [n for n in H if H.in_degree(n)==0]\n",
    "    dist  = {n: 0.0 for n in roots}\n",
    "    prev  = {}\n",
    "    pq    = [(0.0, n) for n in roots]\n",
    "\n",
    "    while pq:\n",
    "        cost, u = heappop(pq)\n",
    "        if u == skill_id:\n",
    "            # reconstruct path\n",
    "            path = []\n",
    "            while u in prev:\n",
    "                path.append(u)\n",
    "                u = prev[u]\n",
    "            path.append(u)\n",
    "            return cost, list(reversed(path))\n",
    "        for _, v, d in H.out_edges(u, data=True):\n",
    "            alt = cost + d[\"cost\"]\n",
    "            if alt < dist.get(v, float(\"inf\")):\n",
    "                dist[v] = alt\n",
    "                prev[v] = u\n",
    "                heappush(pq, (alt, v))\n",
    "    return None, None  # unreachable\n",
    "\n",
    "# ─── 3. Report for each required skill of the job ──────────────────────────\n",
    "def report_job(job_key):\n",
    "    jid = name2id.get(job_key.lower())\n",
    "    if jid is None:\n",
    "        raise KeyError(f\"Job '{job_key}' not found in props\")\n",
    "\n",
    "    # gather direct requires (in-edges or out-edges fallback)\n",
    "    req = {src for src,_ in G.in_edges(jid)} or \\\n",
    "          {dst for _,dst in G.out_edges(jid)}\n",
    "\n",
    "    print(f\"\\n=== {job_key.upper()} requires {len(req)} skills ===\\n\")\n",
    "    for sid in sorted(req, key=lambda x: id2name.get(x, \"\").lower()):\n",
    "        cost, path = best_path_to_skill(sid)\n",
    "        skill = id2name.get(sid, f\"<{sid}>\")\n",
    "        if path is None:\n",
    "            print(f\"✘ {skill:30} — no prereq path in H\\n\")\n",
    "            continue\n",
    "\n",
    "        names = \" → \".join(id2name[n] for n in path)\n",
    "        if len(path) == 1:\n",
    "            print(f\"• {skill:30}  (no prerequisites)\")\n",
    "        else:\n",
    "            print(f\"• {skill:30}  steps={len(path)-1} cost={cost:.3f}\")\n",
    "            print(\"    \", names)\n",
    "        print()\n",
    "\n",
    "# ─── Example run ───────────────────────────────────────────────────────────\n",
    "report_job(\"machine learning engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====  MACHINE LEARNING ENGINEER  – 30 required skills  =====\n",
      "\n",
      "• algorithms                      (no prerequisites)\n",
      "\n",
      "• aws                             (no prerequisites)\n",
      "\n",
      "• c++                             (no prerequisites)\n",
      "\n",
      "• collaboration                  (soft or missing – skipped)\n",
      "\n",
      "• communications                 (soft or missing – skipped)\n",
      "\n",
      "• computer science                steps=2  cost=0.016\n",
      "     arithmetic → computer → computer science \n",
      "\n",
      "• creativity                     (soft or missing – skipped)\n",
      "\n",
      "• data science                    (no prerequisites)\n",
      "\n",
      "• deep learning                   (no prerequisites)\n",
      "\n",
      "• docker                          steps=1  cost=0.678\n",
      "     data science → docker \n",
      "\n",
      "• gcp                             (no prerequisites)\n",
      "\n",
      "• google cloud platform (gcp)     steps=1  cost=0.577\n",
      "     gcp → google cloud platform (gcp) \n",
      "\n",
      "• infrastructure                 (soft or missing – skipped)\n",
      "\n",
      "• innovation                     (soft or missing – skipped)\n",
      "\n",
      "• integration                    (soft or missing – skipped)\n",
      "\n",
      "• java                            (no prerequisites)\n",
      "\n",
      "• kubernetes                      (no prerequisites)\n",
      "\n",
      "• library                         (no prerequisites)\n",
      "\n",
      "• machine learning                steps=2  cost=0.008\n",
      "     arithmetic → computer → machine learning \n",
      "\n",
      "• machine learning methods        (no prerequisites)\n",
      "\n",
      "• management                     (soft or missing – skipped)\n",
      "\n",
      "• operations                     (soft or missing – skipped)\n",
      "\n",
      "• planning                       (soft or missing – skipped)\n",
      "\n",
      "• python                          (no prerequisites)\n",
      "\n",
      "• pytorch                         (no prerequisites)\n",
      "\n",
      "• research                       (soft or missing – skipped)\n",
      "\n",
      "• scalability                     steps=3  cost=0.957\n",
      "     arithmetic → computer programming → software design → scalability \n",
      "\n",
      "• scikit-learn                    (no prerequisites)\n",
      "\n",
      "• software engineering            steps=2  cost=0.016\n",
      "     arithmetic → computer → software engineering \n",
      "\n",
      "• tensorflow                      (no prerequisites)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "#  CONFIG  – edit these two lines only\n",
    "# ---------------------------------------------------------------------------\n",
    "# ── tweakables ───────────────────────────────────────────────────────────────\n",
    "NODES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/nodes_final.csv\")\n",
    "EDGES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/relationships_final.csv\")\n",
    "JOB_TITLE = \"machine learning engineer\"\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd, networkx as nx, math, heapq\n",
    "\n",
    "########################################################################\n",
    "# 1.  Load full graph\n",
    "########################################################################\n",
    "G_full = nx.DiGraph()\n",
    "nodes = pd.read_csv(NODES_CSV)\n",
    "edges = pd.read_csv(EDGES_CSV)\n",
    "\n",
    "for _, r in nodes.iterrows():\n",
    "    G_full.add_node(\n",
    "        int(r[\"nodeId:ID\"]),\n",
    "        label_list = r[\"labels:LABEL\"].split(\";\"),\n",
    "        title      = r.get(\"title\", None),        # may be NaN\n",
    "        name       = r.get(\"name\", None)\n",
    "    )\n",
    "\n",
    "for _, r in edges.iterrows():\n",
    "    G_full.add_edge(\n",
    "        int(r[\":START_ID\"]),\n",
    "        int(r[\":END_ID\"]),\n",
    "        score     = float(r[\"score:float\"]),\n",
    "        predicted = bool(r[\"predicted:boolean\"])\n",
    "    )\n",
    "\n",
    "########################################################################\n",
    "# 2.  Keep only Concept / HardSkill / Technology nodes, drop Jobs\n",
    "########################################################################\n",
    "ALLOWED = {\"Concept\", \"HardSkill\", \"Technology\"}\n",
    "\n",
    "sub_nodes = [\n",
    "    n for n,d in G_full.nodes(data=True)\n",
    "    if any(l in ALLOWED for l in d[\"label_list\"])\n",
    "]\n",
    "G = G_full.subgraph(sub_nodes).copy()\n",
    "\n",
    "########################################################################\n",
    "# 3.  Reverse edges so they flow prerequisite → dependent\n",
    "########################################################################\n",
    "H = G.reverse(copy=True)\n",
    "\n",
    "########################################################################\n",
    "# 4.  Assign edge cost = |score| + 0.5·predicted\n",
    "########################################################################\n",
    "for u, v, d in H.edges(data=True):\n",
    "    d[\"cost\"] = abs(G_full[v][u][\"score\"]) + (0.5 if G_full[v][u][\"predicted\"] else 0.0)\n",
    "\n",
    "########################################################################\n",
    "# 5.  Build label maps from original node data\n",
    "########################################################################\n",
    "def label(nid):\n",
    "    d = G_full.nodes[nid]\n",
    "    return d[\"name\"] or d[\"title\"] or f\"<{nid}>\"\n",
    "\n",
    "name2id = {label(n).lower(): n for n in G_full}\n",
    "id2name = {n: label(n) for n in G_full}\n",
    "\n",
    "########################################################################\n",
    "# 6.  Find the Job node and its required skills\n",
    "########################################################################\n",
    "job_id = name2id.get(JOB_TITLE.lower())\n",
    "if job_id is None:\n",
    "    raise KeyError(f\"Job '{JOB_TITLE}' not found\")\n",
    "\n",
    "required = {dst for _, dst in G_full.out_edges(job_id)}          # Job → Skill\n",
    "\n",
    "########################################################################\n",
    "# 7.  Roots (nodes with no incoming prereq edges)\n",
    "########################################################################\n",
    "roots = [n for n in H if H.in_degree(n) == 0]\n",
    "\n",
    "########################################################################\n",
    "# 8.  DP: best predecessor + cost for every node\n",
    "########################################################################\n",
    "best_cost = {n: 0.0 for n in roots}\n",
    "parent    = {}\n",
    "\n",
    "for n in nx.topological_sort(H):\n",
    "    for _, child, d in H.out_edges(n, data=True):\n",
    "        alt = best_cost[n] + d[\"cost\"]\n",
    "        if alt < best_cost.get(child, math.inf):\n",
    "            best_cost[child] = alt\n",
    "            parent[child]    = n\n",
    "\n",
    "########################################################################\n",
    "# 9.  Function to back-track a path\n",
    "########################################################################\n",
    "def path_to(node):\n",
    "    if node not in best_cost:                  # unreachable\n",
    "        return None\n",
    "    path = [node]\n",
    "    while node in parent:\n",
    "        node = parent[node]\n",
    "        path.append(node)\n",
    "    return list(reversed(path))\n",
    "\n",
    "########################################################################\n",
    "# 10.  Print one best path (concepts + hard skills only) per required skill\n",
    "########################################################################\n",
    "print(f\"\\n=====  {JOB_TITLE.upper()}  – {len(required)} required skills  =====\\n\")\n",
    "for sid in sorted(required, key=lambda x: id2name[x].lower()):\n",
    "    if sid not in H:                           # e.g. SoftSkill → skip\n",
    "        print(f\"• {id2name[sid]:30} (soft or missing – skipped)\\n\")\n",
    "        continue\n",
    "\n",
    "    p = path_to(sid)\n",
    "    if p is None:\n",
    "        print(f\"✘ {id2name[sid]:30}  — no prerequisite chain in data\\n\")\n",
    "        continue\n",
    "\n",
    "    names = \" → \".join(id2name[n] for n in p)\n",
    "    if len(p) == 1:\n",
    "        print(f\"• {id2name[sid]:30}  (no prerequisites)\\n\")\n",
    "    else:\n",
    "        print(f\"• {id2name[sid]:30}  steps={len(p)-1}  cost={best_cost[sid]:.3f}\")\n",
    "        print(\"    \", names, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLOPS ENGINEER – 29 required skills\n",
      "\n",
      "✘ automation                     — no Concept prereqs in graph\n",
      "\n",
      "✘ aws                            — no Concept prereqs in graph\n",
      "     • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → operating system → interprocess communication → message passing interface\n",
      "     • amazon lex\n",
      "     • sagemaker\n",
      "\n",
      "✘ azure                          — no Concept prereqs in graph\n",
      "     • foundations of mathematics → mathematics → probability → descriptive statistics and hypothesis testing → maximum likelihood estimation\n",
      "     • ai tools\n",
      "     • jira\n",
      "     • llms\n",
      "     • microsoft azure\n",
      "\n",
      "• computer science                depth=6\n",
      "     foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science \n",
      "\n",
      "✘ containerization               — no Concept prereqs in graph\n",
      "     • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → programming language → type system\n",
      "     • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → cryptography → traffic analysis\n",
      "     • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → operating system → interprocess communication → message passing interface\n",
      "     • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → parallel computing → mapreduce\n",
      "\n",
      "✘ data science                   — no Concept prereqs in graph\n",
      "     • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → mathematical optimization → gradient descent → artificial neural network → backpropagation through time → generative adversarial networks\n",
      "     • ai tools\n",
      "     • docker\n",
      "     • elasticsearch\n",
      "\n",
      "✘ devops                         — no Concept prereqs in graph\n",
      "\n",
      "✘ docker                         — no Concept prereqs in graph\n",
      "\n",
      "✘ gcp                            — no Concept prereqs in graph\n",
      "     • gcp → google cloud platform (gcp)\n",
      "     • google cloud platform\n",
      "     • llms\n",
      "\n",
      "• google cloud platform (gcp)     depth=1\n",
      "     gcp → google cloud platform (gcp) \n",
      "\n",
      "✘ kubernetes                     — no Concept prereqs in graph\n",
      "     • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → operating system → interprocess communication → message passing interface\n",
      "     • api gateway\n",
      "     • elasticsearch\n",
      "     • genai\n",
      "\n",
      "• machine learning                depth=22\n",
      "     foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → entropy → convolutional neural network → statistical methods → fewshot learning → pattern recognition or machine learning → edge detection → corner detection → feature extraction → feature matching → camera calibration or resectioning → epipolar geometry → trifocal tensor → perspective projection → multi view geometry → imaging geometry and physics → image representation → artificial intelligence → machine learning \n",
      "\n",
      "✘ machine learning methods       — no Concept prereqs in graph\n",
      "     • machine learning methods → ensemble classifier\n",
      "     • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → mathematical optimization → gradient descent → artificial neural network → backpropagation through time → generative adversarial networks\n",
      "     • convolutional neural networks → facial recognition\n",
      "     • genai\n",
      "     • llms\n",
      "\n",
      "✘ mlflow                         — no Concept prereqs in graph\n",
      "\n",
      "✘ python                         — no Concept prereqs in graph\n",
      "\n",
      "✘ pytorch                        — no Concept prereqs in graph\n",
      "\n",
      "• scalability                     depth=19\n",
      "     foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → string (computer science) → formal grammar → lexical analysis → parsing → compiler → java (programming language) → software design → scalability \n",
      "\n",
      "✘ tensorflow                     — no Concept prereqs in graph\n",
      "     • fairseq\n",
      "     • llms\n",
      "\n",
      "✘ terraform                      — no Concept prereqs in graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────── Imports & config ────────────────────────────\n",
    "import pandas as pd, networkx as nx, torch\n",
    "from pathlib import Path\n",
    "from numpy.linalg import norm\n",
    "\n",
    "NODES_CSV = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/nodes_final.csv\")\n",
    "EDGES_CSV = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/relationships_final.csv\")\n",
    "EMB_FILE  = Path(\"node_embeddings.pt\")\n",
    "\n",
    "JOB_TITLE   = \"mlops engineer\"        # ← change job here\n",
    "DELTA       = 1.1                   # reward per hop  (>1)\n",
    "ALPHA       = 0.6                   # weight on |score|\n",
    "BETA        = 0.4                   # weight on semantic penalty\n",
    "PRED_PENALTY= 0.5                   # add to |score| if edge.predicted=true\n",
    "END_LABELS  = {\"HardSkill\",\"Technology\"}\n",
    "SUGGEST_K   = 5                  # max Concepts to suggest\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) LOAD\n",
    "nodes_df  = pd.read_csv(NODES_CSV)\n",
    "edges_df  = pd.read_csv(EDGES_CSV)\n",
    "emb       = torch.load(str(EMB_FILE), map_location=\"cpu\")\n",
    "\n",
    "label_of = {int(r[\"nodeId:ID\"]): set(r[\"labels:LABEL\"].split(\";\"))\n",
    "            for _,r in nodes_df.iterrows()}\n",
    "def name_of(nid):\n",
    "    row = nodes_df.loc[nodes_df[\"nodeId:ID\"]==nid].iloc[0]\n",
    "    return row.get(\"name\") or row.get(\"title\") or str(nid)\n",
    "id2name = {n: name_of(n) for n in label_of}\n",
    "name2id = {v.lower(): k for k,v in id2name.items()}\n",
    "\n",
    "job_id = name2id[JOB_TITLE.lower()]\n",
    "Smax   = edges_df[\"score:float\"].abs().max()\n",
    "\n",
    "def cosine(a,b): return float((a@b)/(norm(a)*norm(b)+1e-9))\n",
    "\n",
    "# 2) DAG H = Concept → dependent, with Technology never as dependent\n",
    "H = nx.DiGraph(); H.add_nodes_from(label_of)\n",
    "for _, r in edges_df.iterrows():\n",
    "    dep, prereq = int(r[\":START_ID\"]), int(r[\":END_ID\"])   # CSV direction\n",
    "    if \"Concept\" not in label_of.get(prereq, ()):          # source must be Concept\n",
    "        continue\n",
    "    if \"Technology\" in label_of.get(dep, ()):              # Technology never inside\n",
    "        continue\n",
    "    raw = abs(float(r[\"score:float\"]))\n",
    "    cost = raw + (PRED_PENALTY if r[\"predicted:boolean\"] else 0.0)\n",
    "    if nx.has_path(H, dep, prereq):                        # avoid cycles\n",
    "        continue\n",
    "    if H.has_edge(prereq, dep):\n",
    "        H[prereq][dep][\"cost\"] = min(H[prereq][dep][\"cost\"], cost)\n",
    "        H[prereq][dep][\"raw\"]  = min(H[prereq][dep][\"raw\"],  raw)\n",
    "    else:\n",
    "        H.add_edge(prereq, dep, cost=cost, raw=raw)\n",
    "\n",
    "assert nx.is_directed_acyclic_graph(H)\n",
    "\n",
    "# 3) DP   (deepest, best-cost Concept chain)\n",
    "dp, prev = {n:-1e9 for n in H}, {}\n",
    "for root in [n for n in H if H.in_degree(n)==0 and \"Concept\" in label_of[n]]:\n",
    "    dp[root] = 0.0\n",
    "\n",
    "for u in nx.topological_sort(H):\n",
    "    if dp[u] < -1e8: continue\n",
    "    for _, v, d in H.out_edges(u, data=True):\n",
    "        s_norm  = d[\"raw\"]/Smax if Smax else 0\n",
    "        sem_pen = 1 - cosine(emb[u],emb[v]) if (u in emb and v in emb) else 1\n",
    "        w       = DELTA - ALPHA*s_norm - BETA*sem_pen\n",
    "        if dp[u]+w > dp[v]:\n",
    "            dp[v], prev[v] = dp[u]+w, u\n",
    "\n",
    "def backtrack(n):\n",
    "    if dp.get(n,-1e9)<-1e8: return None\n",
    "    chain=[n]\n",
    "    while chain[-1] in prev: chain.append(prev[chain[-1]])\n",
    "    return list(reversed(chain))\n",
    "\n",
    "# helper: up-to-K Concept parents (never Technology)\n",
    "def suggest_parents(skill):\n",
    "    inc = [p for p,_,_ in H.in_edges(skill) if \"Concept\" in label_of[p]]\n",
    "    if inc:\n",
    "        inc.sort(key=lambda p: H[p][skill][\"cost\"])\n",
    "        return inc[:SUGGEST_K]\n",
    "    # fallback: Concept→skill edges in raw CSV that were filtered\n",
    "    rows = edges_df.loc[edges_df[\":END_ID\"]==skill, \":START_ID\"].astype(int)\n",
    "    cand = [n for n in rows if \"Concept\" in label_of.get(n,())]\n",
    "    return cand[:SUGGEST_K]\n",
    "\n",
    "# 4) REPORT\n",
    "reqs = set(edges_df.loc[edges_df[\":START_ID\"]==job_id, \":END_ID\"].astype(int))\n",
    "print(f\"\\n{JOB_TITLE.upper()} – {len(reqs)} required skills\\n\")\n",
    "\n",
    "for sid in sorted(reqs, key=lambda x:id2name[x].lower()):\n",
    "    if not (label_of[sid] & END_LABELS): continue\n",
    "\n",
    "    chain = backtrack(sid)\n",
    "    if chain and len(chain)>1:\n",
    "        print(f\"• {id2name[sid]:30}  depth={len(chain)-1}\")\n",
    "        print(\"    \", \" → \".join(id2name[n] for n in chain), \"\\n\")\n",
    "        continue\n",
    "\n",
    "    # unreachable: show suggestions (Concepts only)\n",
    "    print(f\"✘ {id2name[sid]:30} — no Concept prereqs in graph\")\n",
    "    for p in suggest_parents(sid):\n",
    "        alt = backtrack(p) or [p]\n",
    "        print(\"     •\", \" → \".join(id2name[n] for n in alt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID for Computer Science: 285\n",
      "Labels on this node: Concept;HardSkill\n",
      "\n",
      "→ First 5 incoming edges (dependent → computer science):\n",
      "   • reinforcement learning ─REQUIRES(-0.135)→ machine learning\n",
      "   • unsupervised learning ─REQUIRES(-0.286)→ machine learning\n",
      "   • document classification ─REQUIRES(-0.521)→ machine learning\n",
      "   • natural language processing ─REQUIRES(-0.318)→ machine learning\n",
      "   • speech recognition ─REQUIRES(-0.032)→ machine learning\n",
      "\n",
      "→ First 5 outgoing edges (computer science → prerequisite):\n",
      "   • machine learning ─REQUIRES(-0.001)→ probability\n",
      "   • machine learning ─REQUIRES(-0.178)→ artificial intelligence\n",
      "   • machine learning ─REQUIRES(-0.083)→ algorithm design\n",
      "   • machine learning ─REQUIRES(-0.036)→ graph theory\n",
      "   • machine learning ─REQUIRES(-0.020)→ computer programming\n"
     ]
    }
   ],
   "source": [
    "# ─── 0. Safe name_of (if you haven’t already) ────────────────────────────\n",
    "def name_of(nid):\n",
    "    row = nodes_df.loc[nodes_df[\"nodeId:ID\"] == nid].iloc[0]\n",
    "    if \"name\" in row and pd.notna(row[\"name\"]):\n",
    "        return row[\"name\"]\n",
    "    elif \"title\" in row and pd.notna(row[\"title\"]):\n",
    "        return row[\"title\"]\n",
    "    else:\n",
    "        return str(nid)\n",
    "\n",
    "# ─── 1. Build id↔name maps ────────────────────────────────────────────────\n",
    "id2name = { int(r[\"nodeId:ID\"]) : name_of(int(r[\"nodeId:ID\"])) \n",
    "            for _,r in nodes_df.iterrows() }\n",
    "name2id = { v.lower(): k for k,v in id2name.items() }\n",
    "\n",
    "# ─── 2. Lookup “Computer Science” ─────────────────────────────────────────\n",
    "cs_id = name2id.get(\"machine learning\")\n",
    "if cs_id is None:\n",
    "    raise KeyError(\"Could not find a node called 'computer science' in name2id\")\n",
    "\n",
    "print(f\"Node ID for Computer Science: {cs_id}\")\n",
    "print(f\"Labels on this node: {nodes_df.loc[nodes_df['nodeId:ID']==cs_id, 'labels:LABEL'].iloc[0]}\\n\")\n",
    "\n",
    "# ─── 3. Show 5 incoming REQUIRES edges ────────────────────────────────────\n",
    "print(\"→ First 5 incoming edges (dependent → computer science):\")\n",
    "inc = edges_df[edges_df[\":END_ID\"] == cs_id].head(5)\n",
    "for _, row in inc.iterrows():\n",
    "    src = int(row[\":START_ID\"])\n",
    "    print(f\"   • {id2name[src]} ─REQUIRES({row['score:float']:.3f})→ {id2name[cs_id]}\")\n",
    "\n",
    "# ─── 4. Show 5 outgoing REQUIRES edges ────────────────────────────────────\n",
    "print(\"\\n→ First 5 outgoing edges (computer science → prerequisite):\")\n",
    "out = edges_df[edges_df[\":START_ID\"] == cs_id].head(5)\n",
    "for _, row in out.iterrows():\n",
    "    tgt = int(row[\":END_ID\"])\n",
    "    print(f\"   • {id2name[cs_id]} ─REQUIRES({row['score:float']:.3f})→ {id2name[tgt]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLOPS ENGINEER — 29 required skills\n",
      "\n",
      "✘ automation                      — no prereq chain in graph\n",
      "\n",
      "✘ aws                             — no prereq chain in graph\n",
      "\n",
      "✘ azure                           — no prereq chain in graph\n",
      "\n",
      "• computer science                depth=6\n",
      "     foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science \n",
      "\n",
      "✘ containerization                — no prereq chain in graph\n",
      "\n",
      "✘ data science                    — no prereq chain in graph\n",
      "\n",
      "✘ devops                          — no prereq chain in graph\n",
      "\n",
      "✘ docker                          — no prereq chain in graph\n",
      "    • data science\n",
      "\n",
      "✘ gcp                             — no prereq chain in graph\n",
      "\n",
      "• google cloud platform (gcp)     depth=1\n",
      "     gcp → google cloud platform (gcp) \n",
      "\n",
      "✘ kubernetes                      — no prereq chain in graph\n",
      "\n",
      "• machine learning                depth=22\n",
      "     foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → entropy → convolutional neural network → statistical methods → fewshot learning → pattern recognition or machine learning → edge detection → corner detection → feature extraction → feature matching → camera calibration or resectioning → epipolar geometry → trifocal tensor → perspective projection → multi view geometry → imaging geometry and physics → image representation → artificial intelligence → machine learning \n",
      "\n",
      "✘ machine learning methods        — no prereq chain in graph\n",
      "\n",
      "✘ mlflow                          — no prereq chain in graph\n",
      "    • continuous improvement process\n",
      "\n",
      "✘ python                          — no prereq chain in graph\n",
      "\n",
      "✘ pytorch                         — no prereq chain in graph\n",
      "\n",
      "• scalability                     depth=19\n",
      "     foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → string (computer science) → formal grammar → lexical analysis → parsing → compiler → java (programming language) → software design → scalability \n",
      "\n",
      "✘ tensorflow                      — no prereq chain in graph\n",
      "\n",
      "✘ terraform                       — no prereq chain in graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────── PATH‐FINDING CELL ─────────────────────────────\n",
    "import pandas as pd, networkx as nx, torch, math\n",
    "from pathlib import Path\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# ─── config ────────────────────────────────────────────────\n",
    "NODES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/nodes_final.csv\")\n",
    "EDGES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/relationships_final.csv\")\n",
    "EMB_FILE    = Path(\"node_embeddings.pt\")\n",
    "\n",
    "JOB         = \"mlops engineer\"                # ← your job title\n",
    "CONCEPT     = \"Concept\"\n",
    "HARDSKILL   = \"HardSkill\"\n",
    "TECH        = \"Technology\"\n",
    "END_LABELS  = {HARDSKILL, TECH}              # only these count as end‐skills\n",
    "SUGGEST_K   = 3                              # how many parents to suggest\n",
    "\n",
    "Δ, α, β, PRED = 1.1, 0.6, 0.4, 0.5            # scoring constants\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) LOAD\n",
    "nodes = pd.read_csv(NODES_CSV)\n",
    "edges = pd.read_csv(EDGES_CSV)\n",
    "emb   = torch.load(str(EMB_FILE), map_location=\"cpu\")\n",
    "\n",
    "label_of = {\n",
    "    int(r[\"nodeId:ID\"]): set(r[\"labels:LABEL\"].split(\";\"))\n",
    "    for _, r in nodes.iterrows()\n",
    "}\n",
    "\n",
    "def name_of(n):\n",
    "    row = nodes.loc[nodes[\"nodeId:ID\"]==n].iloc[0]\n",
    "    return row.get(\"name\") or row.get(\"title\") or str(n)\n",
    "\n",
    "id2name = {n: name_of(n) for n in label_of}\n",
    "name2id = {v.lower(): k for k,v in id2name.items()}\n",
    "\n",
    "job_id = name2id[JOB.lower()]\n",
    "Smax   = edges[\"score:float\"].abs().max()\n",
    "cosine = lambda a,b: float((a@b)/(norm(a)*norm(b)+1e-9))\n",
    "\n",
    "# 2) Build learning‐DAG H: prereq → dependent,\n",
    "#    allow prereq if it’s Concept or HardSkill, but never if Technology.\n",
    "H = nx.DiGraph(); H.add_nodes_from(label_of)\n",
    "for _, r in edges.iterrows():\n",
    "    dep, pre = int(r[\":START_ID\"]), int(r[\":END_ID\"])  # CSV orientation\n",
    "    labs_pre = label_of.get(pre, ())\n",
    "    labs_dep = label_of.get(dep, ())\n",
    "\n",
    "    # prereq must be Concept or HardSkill; dependent must not be Technology\n",
    "    if not (CONCEPT in labs_pre or HARDSKILL in labs_pre):\n",
    "        continue\n",
    "    if TECH in labs_dep:\n",
    "        continue\n",
    "\n",
    "    raw  = abs(float(r[\"score:float\"]))\n",
    "    cost = raw + (PRED if r[\"predicted:boolean\"] else 0.0)\n",
    "\n",
    "    # avoid cycles before adding\n",
    "    if nx.has_path(H, dep, pre):\n",
    "        continue\n",
    "\n",
    "    # insert / tighten\n",
    "    if H.has_edge(pre, dep):\n",
    "        H[pre][dep][\"cost\"] = min(H[pre][dep][\"cost\"], cost)\n",
    "        H[pre][dep][\"raw\"]  = min(H[pre][dep][\"raw\"],  raw)\n",
    "    else:\n",
    "        H.add_edge(pre, dep, cost=cost, raw=raw)\n",
    "\n",
    "assert nx.is_directed_acyclic_graph(H), \"H must be a DAG!\"\n",
    "\n",
    "# 3) DP: deepest + best‐score chain to every node\n",
    "dp, prev = {n:-math.inf for n in H}, {}\n",
    "# seeds: any Concept or HardSkill with zero prereqs\n",
    "for n in H:\n",
    "    if H.in_degree(n)==0 and (CONCEPT in label_of[n] or HARDSKILL in label_of[n]):\n",
    "        dp[n] = 0.0\n",
    "\n",
    "for u in nx.topological_sort(H):\n",
    "    if dp[u] < -1e8:\n",
    "        continue\n",
    "    for _, v, d in H.out_edges(u, data=True):\n",
    "        s_norm  = d[\"raw\"] / Smax if Smax else 0.0\n",
    "        sem_pen = 1 - cosine(emb[u], emb[v]) if (u in emb and v in emb) else 1.0\n",
    "        gain    = Δ - α*s_norm - β*sem_pen\n",
    "        cand    = dp[u] + gain\n",
    "        if cand > dp[v]:\n",
    "            dp[v], prev[v] = cand, u\n",
    "\n",
    "def build_chain(n):\n",
    "    if dp.get(n, -math.inf) < -1e8:\n",
    "        return None\n",
    "    path = [n]\n",
    "    while path[-1] in prev:\n",
    "        path.append(prev[path[-1]])\n",
    "    return list(reversed(path))\n",
    "\n",
    "# 4) Suggestions for orphan skills\n",
    "def suggest_parents(skill):\n",
    "    # raw CSV: skill → prereq; we want only Concept/HardSkill prereqs\n",
    "    rows = edges.loc[edges[\":START_ID\"] == skill, \":END_ID\"].astype(int)\n",
    "    cands = [p for p in rows if (CONCEPT in label_of.get(p,()) or HARDSKILL in label_of.get(p,()))]\n",
    "    # rank by raw cost\n",
    "    scored = []\n",
    "    for p in cands:\n",
    "        r = edges[(edges[\":START_ID\"]==skill)&(edges[\":END_ID\"]==p)].iloc[0]\n",
    "        sc = abs(float(r[\"score:float\"])) + (PRED if r[\"predicted:boolean\"] else 0.0)\n",
    "        scored.append((p, sc))\n",
    "    scored.sort(key=lambda x: x[1])\n",
    "    return [p for p,_ in scored[:SUGGEST_K]]\n",
    "\n",
    "# 5) REPORT for required skills\n",
    "reqs = set(edges.loc[edges[\":START_ID\"]==job_id, \":END_ID\"].astype(int))\n",
    "print(f\"\\n{JOB.upper()} — {len(reqs)} required skills\\n\")\n",
    "\n",
    "for sid in sorted(reqs, key=lambda x:id2name[x].lower()):\n",
    "    # only HardSkill/Technology as end targets\n",
    "    if not (label_of[sid] & END_LABELS):\n",
    "        continue\n",
    "\n",
    "    chain = build_chain(sid)\n",
    "    if chain and len(chain) > 1:\n",
    "        print(f\"• {id2name[sid]:30}  depth={len(chain)-1}\")\n",
    "        print(\"    \", \" → \".join(id2name[n] for n in chain), \"\\n\")\n",
    "    else:\n",
    "        print(f\"✘ {id2name[sid]:30}  — no prereq chain in graph\")\n",
    "        for p in suggest_parents(sid):\n",
    "            alt = build_chain(p) or [p]\n",
    "            print(\"    •\", \" → \".join(id2name[n] for n in alt))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMPUTER VISION ENGINEER — 29 required skills\n",
      "\n",
      "✘ algorithms                      — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • genai\n",
      "    • llms\n",
      "    • algorithms → address locator\n",
      "\n",
      "✘ aws                             — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • sagemaker\n",
      "    • amazon lex\n",
      "    • graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → operating system → interprocess communication → message passing interface\n",
      "\n",
      "✘ c++                             — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → operating system → interprocess communication → message passing interface\n",
      "    • graph theory → computer science → computability → algorithm → p versus np problem\n",
      "    • graph theory → computer science → computability → algorithm → genetic algorithm\n",
      "\n",
      "• computer science                depth=1\n",
      "     graph theory → computer science \n",
      "\n",
      "• computer vision                 (already known)\n",
      "\n",
      "✘ deep learning                   — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • llms\n",
      "\n",
      "✘ docker                          — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "• image processing                depth=25\n",
      "     foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → entropy → convolutional neural network → statistical methods → fewshot learning → pattern recognition or machine learning → edge detection → corner detection → feature extraction → feature matching → camera calibration or resectioning → epipolar geometry → trifocal tensor → perspective projection → multi view geometry → imaging geometry and physics → image representation → artificial intelligence → segmentation → background modeling and update → background subtraction → image processing \n",
      "\n",
      "✘ keras                           — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ kubernetes                      — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • elasticsearch\n",
      "    • graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → operating system → interprocess communication → message passing interface\n",
      "    • api gateway\n",
      "\n",
      "• machine learning                depth=22\n",
      "     foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → entropy → convolutional neural network → statistical methods → fewshot learning → pattern recognition or machine learning → edge detection → corner detection → feature extraction → feature matching → camera calibration or resectioning → epipolar geometry → trifocal tensor → perspective projection → multi view geometry → imaging geometry and physics → image representation → artificial intelligence → machine learning \n",
      "\n",
      "✘ machine learning methods        — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • graph theory → computer science → computability → algorithm → mathematical optimization → gradient descent → artificial neural network → backpropagation through time → generative adversarial networks\n",
      "    • llms\n",
      "    • machine learning methods → ensemble classifier\n",
      "\n",
      "• object detection                depth=2\n",
      "     computer vision → image retrieval → object detection \n",
      "\n",
      "✘ opencv                          — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ python                          — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ pytorch                         — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "• robotics                        depth=24\n",
      "     foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → entropy → convolutional neural network → statistical methods → fewshot learning → pattern recognition or machine learning → edge detection → corner detection → feature extraction → feature matching → camera calibration or resectioning → epipolar geometry → trifocal tensor → perspective projection → multi view geometry → imaging geometry and physics → image representation → artificial intelligence → machine learning → speech recognition → robotics \n",
      "\n",
      "✘ sql                             — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • graph theory → computer science → computability → algorithm → algorithm design → computational complexity theory → computer programming → programming language → data type → relational model → relational database → index (database) → query optimization\n",
      "\n",
      "✘ tensorflow                      — no prereq chain in graph\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • llms\n",
      "    • fairseq\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────── PATH-FINDING CELL ──────────────────────────────\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import math\n",
    "from pathlib import Path\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# ─── config ────────────────────────────────────────────────────────────────────\n",
    "NODES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/nodes_final.csv\")\n",
    "EDGES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/relationships_final.csv\")\n",
    "EMB_FILE    = Path(\"node_embeddings.pt\")\n",
    "\n",
    "JOB         = \"computer vision engineer\"                # ← your job title\n",
    "CONCEPT     = \"Concept\"\n",
    "HARDSKILL   = \"HardSkill\"\n",
    "TECH        = \"Technology\"\n",
    "END_LABELS  = {HARDSKILL, TECH}              # only these count as end-skills\n",
    "SUGGEST_K   = 3                              # how many concepts to suggest for orphans\n",
    "\n",
    "Δ, α, β, PRED = 0.3, 0.9, 0.1, 0.7        # scoring constants\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) LOAD CSVs & EMBEDDINGS\n",
    "nodes = pd.read_csv(NODES_CSV)\n",
    "edges = pd.read_csv(EDGES_CSV)\n",
    "emb   = torch.load(str(EMB_FILE), map_location=\"cpu\")\n",
    "\n",
    "# build label lookup\n",
    "label_of = {\n",
    "    int(r[\"nodeId:ID\"]): set(r[\"labels:LABEL\"].split(\";\"))\n",
    "    for _, r in nodes.iterrows()\n",
    "}\n",
    "\n",
    "# safe name lookup\n",
    "def name_of(n):\n",
    "    row = nodes.loc[nodes[\"nodeId:ID\"] == n].iloc[0]\n",
    "    if \"name\" in row and pd.notna(row[\"name\"]):\n",
    "        return row[\"name\"]\n",
    "    if \"title\" in row and pd.notna(row[\"title\"]):\n",
    "        return row[\"title\"]\n",
    "    return str(n)\n",
    "\n",
    "# id↔name maps\n",
    "id2name = {nid: name_of(nid) for nid in label_of}\n",
    "name2id = {n.lower(): nid for nid, n in id2name.items()}\n",
    "\n",
    "job_id = name2id.get(JOB.lower())\n",
    "if job_id is None:\n",
    "    raise KeyError(f\"Job '{JOB}' not found in nodes\")\n",
    "Smax = edges[\"score:float\"].abs().max()\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(u, v):\n",
    "    return float((u @ v) / (norm(u) * norm(v) + 1e-9))\n",
    "\n",
    "# 2) BUILD learning-DAG H: prereq → dependent\n",
    "H = nx.DiGraph()\n",
    "H.add_nodes_from(label_of.keys())\n",
    "\n",
    "for _, r in edges.iterrows():\n",
    "    dep, pre = int(r[\":START_ID\"]), int(r[\":END_ID\"])  # CSV: dependent→prereq\n",
    "    labs_pre = label_of.get(pre, ())\n",
    "    labs_dep = label_of.get(dep, ())\n",
    "\n",
    "    # only keep if prereq is Concept or HardSkill, and dependent isn't Technology\n",
    "    if not (CONCEPT in labs_pre or HARDSKILL in labs_pre):\n",
    "        continue\n",
    "    if TECH in labs_dep:\n",
    "        continue\n",
    "\n",
    "    raw  = abs(float(r[\"score:float\"]))\n",
    "    cost = raw + (PRED if r[\"predicted:boolean\"] else 0.0)\n",
    "\n",
    "    # avoid cycles\n",
    "    if nx.has_path(H, dep, pre):\n",
    "        continue\n",
    "\n",
    "    # insert or tighten existing edge\n",
    "    if H.has_edge(pre, dep):\n",
    "        H[pre][dep][\"cost\"] = min(H[pre][dep][\"cost\"], cost)\n",
    "        H[pre][dep][\"raw\"]  = min(H[pre][dep][\"raw\"],  raw)\n",
    "    else:\n",
    "        H.add_edge(pre, dep, cost=cost, raw=raw)\n",
    "\n",
    "assert nx.is_directed_acyclic_graph(H), \"H must be a DAG!\"\n",
    "\n",
    "# 3) DP: longest + best-score path to every node\n",
    "dp   = {n: -math.inf for n in H}\n",
    "prev = {}\n",
    "\n",
    "# seed roots: any Concept/HardSkill with zero prerequisites\n",
    "for n in H:\n",
    "    if H.in_degree(n) == 0 and (CONCEPT in label_of[n] or HARDSKILL in label_of[n]):\n",
    "        dp[n] = 0.0\n",
    "\n",
    "for u in nx.topological_sort(H):\n",
    "    if dp[u] < -1e8:\n",
    "        continue\n",
    "    for _, v, d in H.out_edges(u, data=True):\n",
    "        s_norm  = d[\"raw\"] / Smax if Smax else 0.0\n",
    "        sem_pen = (1 - cosine(emb[u], emb[v])) if (u in emb and v in emb) else 1.0\n",
    "        gain    = Δ - α * s_norm - β * sem_pen\n",
    "        cand    = dp[u] + gain\n",
    "        if cand > dp[v]:\n",
    "            dp[v], prev[v] = cand, u\n",
    "\n",
    "# back-track chain builder\n",
    "def build_chain(n):\n",
    "    if dp.get(n, -math.inf) < -1e8:\n",
    "        return None\n",
    "    path = [n]\n",
    "    while path[-1] in prev:\n",
    "        path.append(prev[path[-1]])\n",
    "    return list(reversed(path))\n",
    "\n",
    "# ─────────── USER INPUT ──────────────────────────────────\n",
    "# ---------------- USER INPUT -----------------\n",
    "user_skills = [\"graph theory\", \"computer vision\",\"neural network\",\"reinforcement learning\"]          # ← whatever they enter\n",
    "user_ids    = { name2id[s.lower()] for s in user_skills if s.lower() in name2id }\n",
    "# ---------------------------------------------\n",
    "\n",
    "def make_filtered_graph(target_id):\n",
    "    \"\"\"Subgraph of H with cosine≥threshold *plus* all roots *plus* user skills.\"\"\"\n",
    "    target_emb = emb.get(target_id)\n",
    "    roots = {n for n in H if H.in_degree(n)==0\n",
    "                          and (CONCEPT in label_of[n] or HARDSKILL in label_of[n])}\n",
    "\n",
    "    good = set(roots) | {target_id} | user_ids        # <- keep user skills!\n",
    "\n",
    "    if target_emb is not None:\n",
    "        for n in H:\n",
    "            if n in emb and (CONCEPT in label_of[n] or HARDSKILL in label_of[n]):\n",
    "                if cosine(emb[n], target_emb) >= DOMAIN_THRESH:\n",
    "                    good.add(n)\n",
    "\n",
    "    return H.subgraph(good).copy()\n",
    "\n",
    "# helper: trim a path so it starts with the first node the user already knows\n",
    "def trim_to_user(path):\n",
    "    for i, n in enumerate(path):\n",
    "        if n in user_ids:\n",
    "            return path[i:]\n",
    "    return None          # no user skill inside\n",
    "\n",
    "# ─────────── 5. REPORT (replaces your current loop) ──────\n",
    "reqs = set(edges.loc[edges[\":START_ID\"] == job_id, \":END_ID\"].astype(int))\n",
    "print(f\"\\n{JOB.upper()} — {len(reqs)} required skills\\n\")\n",
    "\n",
    "for sid in sorted(reqs, key=lambda x: id2name[x].lower()):\n",
    "    if not (label_of[sid] & END_LABELS):\n",
    "        continue  # ignore SoftSkills, Jobs, etc.\n",
    "\n",
    "    chain = build_chain(sid)\n",
    "\n",
    "    # 1) If chain exists, try to anchor it at a user skill\n",
    "    if chain and len(chain) > 1:\n",
    "        anchored = trim_to_user(chain) if user_ids else None\n",
    "        final_chain = anchored if anchored else chain\n",
    "\n",
    "        if len(final_chain) == 1:          # skill already known\n",
    "            print(f\"• {id2name[sid]:30}  (already known)\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"• {id2name[sid]:30}  depth={len(final_chain)-1}\")\n",
    "        print(\"    \", \" → \".join(id2name[n] for n in final_chain), \"\\n\")\n",
    "        continue\n",
    "\n",
    "    # 2) Orphan skill: show dependents to learn next\n",
    "    print(f\"✘ {id2name[sid]:30}  — no prereq chain in graph\")\n",
    "    deps = edges.loc[edges[\":END_ID\"] == sid, \":START_ID\"].astype(int)\n",
    "    cands = [d for d in deps if (CONCEPT in label_of.get(d,()) or HARDSKILL in label_of.get(d,()))]\n",
    "\n",
    "    scored = []\n",
    "    for d in cands:\n",
    "        r = edges[(edges[\":START_ID\"]==d)&(edges[\":END_ID\"]==sid)].iloc[0]\n",
    "        sc = abs(float(r[\"score:float\"])) + (PRED if r[\"predicted:boolean\"] else 0.0)\n",
    "        scored.append((d, sc))\n",
    "    scored.sort(key=lambda x: x[1])\n",
    "\n",
    "    print(\"    Concepts that depend on this skill (learn next):\")\n",
    "    for d, _ in scored[:SUGGEST_K]:\n",
    "        alt = build_chain(d) or [d]\n",
    "        # anchor each alt chain at user skill if possible\n",
    "        anchored = trim_to_user(alt) if user_ids else None\n",
    "        final_alt = anchored if anchored else alt\n",
    "        print(\"    •\", \" → \".join(id2name[n] for n in final_alt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 edges to fill structural gaps\n",
      "\n",
      "AI ARCHITECT — 31 required skills\n",
      "\n",
      "\n",
      "=== SKILLS YOU ALREADY KNOW ===\n",
      "\n",
      "=== SKILLS READY TO LEARN ===\n",
      "\n",
      "=== SKILLS REQUIRING PREREQUISITES ===\n",
      "• artificial intelligence         depth=4\n",
      "     radiance → specular surfaces → texture → feature extraction → artificial intelligence \n",
      "\n",
      "• computer science                depth=2\n",
      "     foundations of mathematics → mathematics → computer science \n",
      "\n",
      "• google cloud platform (gcp)     depth=1\n",
      "     gcp → google cloud platform (gcp) \n",
      "\n",
      "• scalability                     depth=5\n",
      "     foundations of mathematics → mathematics → computer science → computer programming → software design → scalability \n",
      "\n",
      "• software development            depth=3\n",
      "     foundations of mathematics → mathematics → computer science → software development \n",
      "\n",
      "\n",
      "=== ORPHAN SKILLS WITH CONNECTIONS TO YOUR KNOWLEDGE ===\n",
      "\n",
      "=== ORPHAN SKILLS ===\n",
      "✘ ai software                     — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ algorithms                      — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • genai\n",
      "    • llms\n",
      "    • algorithms → address locator\n",
      "    • data → interprocess communication → message passing interface\n",
      "    • tensorrt\n",
      "\n",
      "✘ analytics                       — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • ai tools\n",
      "    • llms\n",
      "\n",
      "✘ architectural design            — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • probability → generative adversarial networks\n",
      "    • foundations of mathematics → mathematics → computer science → data structure → database design\n",
      "\n",
      "✘ artificial intelligence markup language (aiml)  — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ confluence                      — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ data science                    — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • probability → generative adversarial networks\n",
      "    • docker\n",
      "    • elasticsearch\n",
      "    • ai tools\n",
      "\n",
      "✘ devops                          — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ jira                            — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ langchain                       — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ machine learning methods        — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • probability → generative adversarial networks\n",
      "    • llms\n",
      "    • machine learning methods → ensemble classifier\n",
      "    • genai\n",
      "    • rllib\n",
      "\n",
      "✘ python                          — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ pytorch                         — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ tableau                         — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ tensorflow                      — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • llms\n",
      "    • fairseq\n",
      "\n",
      "✘ translations                    — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────── PATH-FINDING CELL ──────────────────────────────\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import math\n",
    "from pathlib import Path\n",
    "from numpy.linalg import norm\n",
    "import heapq\n",
    "from collections import Counter\n",
    "\n",
    "# ─── config ────────────────────────────────────────────────────────────────────\n",
    "NODES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/nodes_final.csv\")\n",
    "EDGES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/relationships_final.csv\")\n",
    "EMB_FILE    = Path(\"node_embeddings.pt\")\n",
    "\n",
    "JOB         = \"ai architect\"                # ← your job title\n",
    "CONCEPT     = \"Concept\"\n",
    "HARDSKILL   = \"HardSkill\"\n",
    "TECH        = \"Technology\"\n",
    "END_LABELS  = {HARDSKILL, TECH}              # only these count as end-skills\n",
    "SUGGEST_K   = 5                              # how many concepts to suggest for orphans\n",
    "MAX_PATH_LENGTH = 15                     # maximum reasonable path length\n",
    "\n",
    "# Scoring parameters\n",
    "Δ, α, β, PRED = 0.3, 0.9, 0.1, 0.7        # scoring constants\n",
    "DOMAIN_THRESH = 0.5                        # semantic similarity threshold\n",
    "SIM_BOOST = 0.6                         # boost for semantically similar nodes\n",
    "LENGTH_PENALTY = 0.05                  # penalty for longer paths\n",
    "ORPHAN_SIM_THRESHOLD = 0.3            # threshold for connecting orphan skills to user skills\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) LOAD CSVs & EMBEDDINGS\n",
    "nodes = pd.read_csv(NODES_CSV)\n",
    "edges = pd.read_csv(EDGES_CSV)\n",
    "emb   = torch.load(str(EMB_FILE), map_location=\"cpu\")\n",
    "\n",
    "# build label lookup\n",
    "label_of = {\n",
    "    int(r[\"nodeId:ID\"]): set(r[\"labels:LABEL\"].split(\";\"))\n",
    "    for _, r in nodes.iterrows()\n",
    "}\n",
    "\n",
    "# safe name lookup\n",
    "def name_of(n):\n",
    "    row = nodes.loc[nodes[\"nodeId:ID\"] == n].iloc[0]\n",
    "    if \"name\" in row and pd.notna(row[\"name\"]):\n",
    "        return row[\"name\"]\n",
    "    if \"title\" in row and pd.notna(row[\"title\"]):\n",
    "        return row[\"title\"]\n",
    "    return str(n)\n",
    "\n",
    "# id↔name maps\n",
    "id2name = {nid: name_of(nid) for nid in label_of}\n",
    "name2id = {n.lower(): nid for nid, n in id2name.items()}\n",
    "\n",
    "job_id = name2id.get(JOB.lower())\n",
    "if job_id is None:\n",
    "    raise KeyError(f\"Job '{JOB}' not found in nodes\")\n",
    "Smax = edges[\"score:float\"].abs().max()\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(u, v):\n",
    "    return float((u @ v) / (norm(u) * norm(v) + 1e-9))\n",
    "\n",
    "# 2) BUILD learning-DAG H: prereq → dependent\n",
    "H = nx.DiGraph()\n",
    "H.add_nodes_from(label_of.keys())\n",
    "\n",
    "# Extract all edge information first to help identify important relationships\n",
    "edge_counts = Counter()\n",
    "for _, r in edges.iterrows():\n",
    "    dep, pre = int(r[\":START_ID\"]), int(r[\":END_ID\"])  # CSV: dependent→prereq\n",
    "    edge_counts[(dep, pre)] += 1\n",
    "\n",
    "for _, r in edges.iterrows():\n",
    "    dep, pre = int(r[\":START_ID\"]), int(r[\":END_ID\"])  # CSV: dependent→prereq\n",
    "    labs_pre = label_of.get(pre, ())\n",
    "    labs_dep = label_of.get(dep, ())\n",
    "\n",
    "    # only keep if prereq is Concept or HardSkill, and dependent isn't Technology\n",
    "    if not (CONCEPT in labs_pre or HARDSKILL in labs_pre):\n",
    "        continue\n",
    "    if TECH in labs_dep:\n",
    "        continue\n",
    "\n",
    "    raw  = abs(float(r[\"score:float\"]))\n",
    "    \n",
    "    # Give small boost to frequently occurring edges - indicates importance\n",
    "    importance_factor = min(1.0 + 0.05 * edge_counts[(dep, pre)], 1.3)\n",
    "    raw = raw * importance_factor\n",
    "    \n",
    "    cost = raw + (PRED if r[\"predicted:boolean\"] else 0.0)\n",
    "\n",
    "    # Semantic similarity boost\n",
    "    if pre in emb and dep in emb:\n",
    "        sim = cosine(emb[pre], emb[dep])\n",
    "        if sim > 0.7:  # High similarity\n",
    "            cost = cost * (1.0 - SIM_BOOST)  # Lower cost for similar concepts\n",
    "\n",
    "    # avoid cycles\n",
    "    if nx.has_path(H, dep, pre):\n",
    "        continue\n",
    "\n",
    "    # insert or tighten existing edge\n",
    "    if H.has_edge(pre, dep):\n",
    "        H[pre][dep][\"cost\"] = min(H[pre][dep][\"cost\"], cost)\n",
    "        H[pre][dep][\"raw\"]  = min(H[pre][dep][\"raw\"],  raw)\n",
    "    else:\n",
    "        H.add_edge(pre, dep, cost=cost, raw=raw)\n",
    "\n",
    "assert nx.is_directed_acyclic_graph(H), \"H must be a DAG!\"\n",
    "\n",
    "# Find structural gaps in the knowledge graph\n",
    "def detect_and_fill_gaps():\n",
    "    \"\"\"\n",
    "    Detect structural gaps in the graph by finding skills that should be connected\n",
    "    based on semantic similarity but aren't linked in the graph.\n",
    "    \"\"\"\n",
    "    tech_nodes = [n for n in H.nodes() if TECH in label_of.get(n, set())]\n",
    "    skills_nodes = [n for n in H.nodes() if HARDSKILL in label_of.get(n, set())]\n",
    "    concept_nodes = [n for n in H.nodes() if CONCEPT in label_of.get(n, set())]\n",
    "    \n",
    "    additions = 0\n",
    "    \n",
    "    # Look for technologies that should connect to relevant skills\n",
    "    for tech in tech_nodes:\n",
    "        if tech not in emb:\n",
    "            continue\n",
    "            \n",
    "        # Find semantically similar skills or concepts\n",
    "        similar_nodes = []\n",
    "        for skill in skills_nodes + concept_nodes:\n",
    "            if skill not in emb:\n",
    "                continue\n",
    "                \n",
    "            sim = cosine(emb[tech], emb[skill])\n",
    "            if sim > 0.65:  # High similarity threshold\n",
    "                similar_nodes.append((skill, sim))\n",
    "        \n",
    "        # Sort by similarity and connect the top matches if not already connected\n",
    "        similar_nodes.sort(key=lambda x: x[1], reverse=True)\n",
    "        for skill, sim in similar_nodes[:3]:  # Top 3 most similar\n",
    "            if not H.has_edge(skill, tech) and not nx.has_path(H, tech, skill):\n",
    "                # Add edge from skill to tech (skill is prerequisite for tech)\n",
    "                H.add_edge(skill, tech, cost=0.6, raw=0.6)\n",
    "                additions += 1\n",
    "    \n",
    "    # Connect isolated skills to relevant concepts\n",
    "    orphan_skills = [n for n in H.nodes() \n",
    "                    if HARDSKILL in label_of.get(n, set()) \n",
    "                    and H.in_degree(n) == 0\n",
    "                    and H.out_degree(n) == 0]\n",
    "    \n",
    "    for orphan in orphan_skills:\n",
    "        if orphan not in emb:\n",
    "            continue\n",
    "            \n",
    "        # Find relevant concepts\n",
    "        similar_concepts = []\n",
    "        for concept in concept_nodes:\n",
    "            if concept not in emb:\n",
    "                continue\n",
    "                \n",
    "            sim = cosine(emb[orphan], emb[concept])\n",
    "            if sim > 0.6:\n",
    "                similar_concepts.append((concept, sim))\n",
    "        \n",
    "        # Connect to top matches\n",
    "        similar_concepts.sort(key=lambda x: x[1], reverse=True)\n",
    "        for concept, sim in similar_concepts[:2]:  # Top 2\n",
    "            # Add bidirectional edges to represent soft relationships\n",
    "            if not nx.has_path(H, orphan, concept) and not nx.has_path(H, concept, orphan):\n",
    "                H.add_edge(concept, orphan, cost=0.7, raw=0.7)\n",
    "                additions += 1\n",
    "    \n",
    "    print(f\"Added {additions} edges to fill structural gaps\")\n",
    "    return additions\n",
    "\n",
    "# Fill graph gaps\n",
    "detect_and_fill_gaps()\n",
    "\n",
    "# 3) DP: longest + best-score path to every node with improved path quality\n",
    "dp   = {n: -math.inf for n in H}\n",
    "prev = {}\n",
    "path_lens = {n: 0 for n in H}  # Keep track of path lengths\n",
    "\n",
    "# seed roots: any Concept/HardSkill with zero prerequisites\n",
    "for n in H:\n",
    "    if H.in_degree(n) == 0 and (CONCEPT in label_of[n] or HARDSKILL in label_of[n]):\n",
    "        dp[n] = 0.0\n",
    "        path_lens[n] = 1\n",
    "\n",
    "# Improved DP prioritizing path quality and reasonable length\n",
    "for u in nx.topological_sort(H):\n",
    "    if dp[u] < -1e8:\n",
    "        continue\n",
    "    for _, v, d in H.out_edges(u, data=True):\n",
    "        s_norm  = d[\"raw\"] / Smax if Smax else 0.0\n",
    "        sem_pen = (1 - cosine(emb[u], emb[v])) if (u in emb and v in emb) else 1.0\n",
    "        \n",
    "        # Add length penalty to discourage excessively long paths\n",
    "        length_factor = LENGTH_PENALTY * path_lens[u]\n",
    "        \n",
    "        gain = Δ - α * s_norm - β * sem_pen - length_factor\n",
    "        cand = dp[u] + gain\n",
    "        \n",
    "        # Only consider if path length is reasonable\n",
    "        new_path_len = path_lens[u] + 1\n",
    "        if new_path_len <= MAX_PATH_LENGTH and cand > dp[v]:\n",
    "            dp[v], prev[v] = cand, u\n",
    "            path_lens[v] = new_path_len\n",
    "\n",
    "# back-track chain builder with length consideration\n",
    "def build_chain(n):\n",
    "    if dp.get(n, -math.inf) < -1e8:\n",
    "        return None\n",
    "    path = [n]\n",
    "    while path[-1] in prev:\n",
    "        path.append(prev[path[-1]])\n",
    "        if len(path) > MAX_PATH_LENGTH:\n",
    "            break  # Enforce maximum path length\n",
    "    return list(reversed(path))\n",
    "\n",
    "# Alternative path finding using A* for shorter, more direct paths\n",
    "def find_alt_path(start, target, max_length=MAX_PATH_LENGTH):\n",
    "    \"\"\"Find alternative path using A* search for more direct routes.\"\"\"\n",
    "    if start not in H or target not in H:\n",
    "        return None\n",
    "        \n",
    "    # A* search with heuristic based on embedding similarity\n",
    "    def heuristic(n):\n",
    "        if n in emb and target in emb:\n",
    "            return 1 - cosine(emb[n], emb[target])  # Lower for similar nodes\n",
    "        return 1.0\n",
    "    \n",
    "    open_set = [(0, 0, start, [start])]  # (f_score, g_score, node, path)\n",
    "    closed_set = set()\n",
    "    \n",
    "    while open_set:\n",
    "        _, g_score, current, path = heapq.heappop(open_set)\n",
    "        \n",
    "        if current == target:\n",
    "            return path\n",
    "            \n",
    "        if current in closed_set or len(path) > max_length:\n",
    "            continue\n",
    "            \n",
    "        closed_set.add(current)\n",
    "        \n",
    "        for _, neighbor in H.out_edges(current):\n",
    "            if neighbor in closed_set:\n",
    "                continue\n",
    "                \n",
    "            tentative_g = g_score + 1\n",
    "            f_score = tentative_g + heuristic(neighbor)\n",
    "            \n",
    "            heapq.heappush(open_set, (f_score, tentative_g, neighbor, path + [neighbor]))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Find connections from user skills to orphans\n",
    "def find_path_to_user_skill(target_id):\n",
    "    \"\"\"Try to find a path from any user skill to the target.\"\"\"\n",
    "    if not user_ids:\n",
    "        return None\n",
    "        \n",
    "    # Try each user skill\n",
    "    best_path = None\n",
    "    best_sim = -1\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        # Direct semantic similarity\n",
    "        if target_id in emb and user_id in emb:\n",
    "            sim = cosine(emb[user_id], emb[target_id])\n",
    "            if sim > ORPHAN_SIM_THRESHOLD and sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_path = [user_id, target_id]\n",
    "        \n",
    "        # Try A* search for a path\n",
    "        path = find_alt_path(user_id, target_id, max_length=3)\n",
    "        if path and len(path) > 1:\n",
    "            path_quality = sum(cosine(emb[path[i]], emb[path[i+1]]) \n",
    "                              for i in range(len(path)-1) \n",
    "                              if path[i] in emb and path[i+1] in emb) / (len(path)-1)\n",
    "            if path_quality > best_sim:\n",
    "                best_sim = path_quality\n",
    "                best_path = path\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "# ─────────── USER INPUT ──────────────────────────────────\n",
    "# ---------------- USER INPUT -----------------\n",
    "user_skills = [\"graph theory\",\"probability\",\"linear algebra\"]\n",
    "user_ids = { name2id[s.lower()] for s in user_skills if s.lower() in name2id }\n",
    "# ---------------------------------------------\n",
    "\n",
    "# helper: trim a path so it starts with the first node the user already knows\n",
    "def trim_to_user(path):\n",
    "    if path is None:\n",
    "        return None\n",
    "    for i, n in enumerate(path):\n",
    "        if n in user_ids:\n",
    "            return path[i:]\n",
    "    return path  # Return the full path if no user skill found\n",
    "\n",
    "# Rate the quality of a path based on relevance to job\n",
    "def path_quality_score(path, job_emb):\n",
    "    \"\"\"Calculate a quality score for the path based on relevance to job.\"\"\"\n",
    "    if not path or len(path) <= 1 or job_emb is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Average similarity to job\n",
    "    job_sim = 0.0\n",
    "    count = 0\n",
    "    for node in path:\n",
    "        if node in emb:\n",
    "            job_sim += cosine(emb[node], job_emb)\n",
    "            count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Penalize very long paths\n",
    "    length_factor = max(0, 1 - 0.03 * (len(path) - 3))\n",
    "    \n",
    "    return (job_sim / count) * length_factor\n",
    "\n",
    "# ─────────── 5. IMPROVED REPORT ──────────────\n",
    "job_emb = emb.get(job_id)\n",
    "reqs = set(edges.loc[edges[\":START_ID\"] == job_id, \":END_ID\"].astype(int))\n",
    "print(f\"\\n{JOB.upper()} — {len(reqs)} required skills\\n\")\n",
    "\n",
    "# Group skills by category for better organization\n",
    "skill_categories = {\n",
    "    \"already_known\": [],      # Skills the user already has\n",
    "    \"ready_to_learn\": [],     # Skills with direct paths from user knowledge\n",
    "    \"requires_prereqs\": [],   # Skills requiring longer paths\n",
    "    \"connected_orphans\": [],  # Orphans with alternative connections\n",
    "    \"true_orphans\": []        # Truly disconnected skills\n",
    "}\n",
    "\n",
    "# Process each required skill\n",
    "for sid in sorted(reqs, key=lambda x: id2name[x].lower()):\n",
    "    if not (label_of[sid] & END_LABELS):\n",
    "        continue  # ignore SoftSkills, Jobs, etc.\n",
    "    \n",
    "    skill_name = id2name[sid]\n",
    "    \n",
    "    # Check if already known\n",
    "    if sid in user_ids:\n",
    "        skill_categories[\"already_known\"].append((sid, [sid]))\n",
    "        continue\n",
    "        \n",
    "    # Try to find optimal path via DP\n",
    "    dp_chain = build_chain(sid)\n",
    "    dp_chain_anchored = trim_to_user(dp_chain) if dp_chain else None\n",
    "    \n",
    "    # If we have a good DP path anchored to user skills\n",
    "    if dp_chain_anchored and len(dp_chain_anchored) > 1 and dp_chain_anchored[0] in user_ids:\n",
    "        if len(dp_chain_anchored) <= 3:\n",
    "            skill_categories[\"ready_to_learn\"].append((sid, dp_chain_anchored))\n",
    "        else:\n",
    "            skill_categories[\"requires_prereqs\"].append((sid, dp_chain_anchored))\n",
    "        continue\n",
    "    \n",
    "    # If we have a DP path but not anchored to user skills\n",
    "    if dp_chain and len(dp_chain) > 1:\n",
    "        # Try to find a shorter alternative path from user skills\n",
    "        alt_path = None\n",
    "        for user_id in user_ids:\n",
    "            user_to_first = find_alt_path(user_id, dp_chain[0], max_length=2)\n",
    "            if user_to_first:\n",
    "                alt_path = user_to_first[:-1] + dp_chain  # Connect without duplicating node\n",
    "                break\n",
    "        \n",
    "        if alt_path:\n",
    "            if len(alt_path) <= 4:\n",
    "                skill_categories[\"ready_to_learn\"].append((sid, alt_path))\n",
    "            else:\n",
    "                skill_categories[\"requires_prereqs\"].append((sid, alt_path))\n",
    "            continue\n",
    "        else:\n",
    "            skill_categories[\"requires_prereqs\"].append((sid, dp_chain))\n",
    "            continue\n",
    "    \n",
    "    # For orphans, try semantic connections to user skills\n",
    "    user_path = find_path_to_user_skill(sid)\n",
    "    if user_path:\n",
    "        skill_categories[\"connected_orphans\"].append((sid, user_path))\n",
    "        continue\n",
    "    \n",
    "    # True orphan - look for dependent skills\n",
    "    deps = edges.loc[edges[\":END_ID\"] == sid, \":START_ID\"].astype(int)\n",
    "    cands = [d for d in deps if (CONCEPT in label_of.get(d,()) or HARDSKILL in label_of.get(d,()))]\n",
    "    \n",
    "    scored = []\n",
    "    for d in cands:\n",
    "        r = edges[(edges[\":START_ID\"]==d)&(edges[\":END_ID\"]==sid)].iloc[0]\n",
    "        sc = abs(float(r[\"score:float\"])) + (PRED if r[\"predicted:boolean\"] else 0.0)\n",
    "        \n",
    "        # Prefer concepts that have paths from user skills\n",
    "        d_chain = build_chain(d)\n",
    "        d_chain_anchored = trim_to_user(d_chain) if d_chain else None\n",
    "        \n",
    "        if d_chain_anchored and d_chain_anchored[0] in user_ids:\n",
    "            sc *= 0.8  # Boost score (lower is better)\n",
    "            \n",
    "        scored.append((d, sc, d_chain_anchored or d_chain))\n",
    "        \n",
    "    scored.sort(key=lambda x: x[1])\n",
    "    skill_categories[\"true_orphans\"].append((sid, [c for c, _, path in scored[:SUGGEST_K]]))\n",
    "\n",
    "# Print results by category\n",
    "print(\"\\n=== SKILLS YOU ALREADY KNOW ===\")\n",
    "for sid, _ in skill_categories[\"already_known\"]:\n",
    "    print(f\"• {id2name[sid]}\")\n",
    "\n",
    "print(\"\\n=== SKILLS READY TO LEARN ===\")\n",
    "for sid, path in skill_categories[\"ready_to_learn\"]:\n",
    "    print(f\"• {id2name[sid]:30}  depth={len(path)-1}\")\n",
    "    print(\"    \", \" → \".join(id2name[n] for n in path), \"\\n\")\n",
    "\n",
    "print(\"\\n=== SKILLS REQUIRING PREREQUISITES ===\")\n",
    "for sid, path in skill_categories[\"requires_prereqs\"]:\n",
    "    print(f\"• {id2name[sid]:30}  depth={len(path)-1}\")\n",
    "    print(\"    \", \" → \".join(id2name[n] for n in path), \"\\n\")\n",
    "\n",
    "print(\"\\n=== ORPHAN SKILLS WITH CONNECTIONS TO YOUR KNOWLEDGE ===\")\n",
    "for sid, path in skill_categories[\"connected_orphans\"]:\n",
    "    print(f\"• {id2name[sid]:30}  (semantic connection)\")\n",
    "    print(\"    \", \" → \".join(id2name[n] for n in path), \"\\n\")\n",
    "\n",
    "print(\"\\n=== ORPHAN SKILLS ===\")\n",
    "for sid, dependents in skill_categories[\"true_orphans\"]:\n",
    "    print(f\"✘ {id2name[sid]:30}  — no direct path from your skills\")\n",
    "    print(\"    Concepts that depend on this skill (learn next):\")\n",
    "    for d in dependents:\n",
    "        alt = build_chain(d) or [d]\n",
    "        anchored = trim_to_user(alt) if user_ids else None\n",
    "        final_alt = anchored if anchored else alt\n",
    "        print(\"    •\", \" → \".join(id2name[n] for n in final_alt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 edges to fill structural gaps\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────── COMMON CODE CELL ──────────────────────────────\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import math\n",
    "from pathlib import Path\n",
    "from numpy.linalg import norm\n",
    "import heapq\n",
    "from collections import Counter\n",
    "\n",
    "# ─── config ────────────────────────────────────────────────────────────────────\n",
    "NODES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/nodes_final.csv\")\n",
    "EDGES_CSV   = Path(\"/teamspace/studios/this_studio/neo4j_export_csv/relationships_final.csv\")\n",
    "EMB_FILE    = Path(\"node_embeddings.pt\")\n",
    "\n",
    "JOB         = \"nlp engineer\"                # ← your job title\n",
    "CONCEPT     = \"Concept\"\n",
    "HARDSKILL   = \"HardSkill\"\n",
    "TECH        = \"Technology\"\n",
    "END_LABELS  = {HARDSKILL, TECH}              # only these count as end-skills\n",
    "SUGGEST_K   = 5                              # how many concepts to suggest for orphans\n",
    "MAX_PATH_LENGTH = 15                       # maximum reasonable path length\n",
    "\n",
    "# Scoring parameters\n",
    "Δ, α, β, PRED = 0.3, 0.9, 0.1, 0.7        # scoring constants\n",
    "DOMAIN_THRESH = 0.7                       # semantic similarity threshold\n",
    "SIM_BOOST = 0.6                            # boost for semantically similar nodes\n",
    "LENGTH_PENALTY = 0.03                      # penalty for longer paths\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) LOAD CSVs & EMBEDDINGS\n",
    "nodes = pd.read_csv(NODES_CSV)\n",
    "edges = pd.read_csv(EDGES_CSV)\n",
    "emb   = torch.load(str(EMB_FILE), map_location=\"cpu\")\n",
    "\n",
    "# build label lookup\n",
    "label_of = {\n",
    "    int(r[\"nodeId:ID\"]): set(r[\"labels:LABEL\"].split(\";\"))\n",
    "    for _, r in nodes.iterrows()\n",
    "}\n",
    "\n",
    "# safe name lookup\n",
    "def name_of(n):\n",
    "    row = nodes.loc[nodes[\"nodeId:ID\"] == n].iloc[0]\n",
    "    if \"name\" in row and pd.notna(row[\"name\"]):\n",
    "        return row[\"name\"]\n",
    "    if \"title\" in row and pd.notna(row[\"title\"]):\n",
    "        return row[\"title\"]\n",
    "    return str(n)\n",
    "\n",
    "# id↔name maps\n",
    "id2name = {nid: name_of(nid) for nid in label_of}\n",
    "name2id = {n.lower(): nid for nid, n in id2name.items()}\n",
    "\n",
    "job_id = name2id.get(JOB.lower())\n",
    "if job_id is None:\n",
    "    raise KeyError(f\"Job '{JOB}' not found in nodes\")\n",
    "Smax = edges[\"score:float\"].abs().max()\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(u, v):\n",
    "    return float((u @ v) / (norm(u) * norm(v) + 1e-9))\n",
    "\n",
    "# 2) BUILD learning-DAG H: prereq → dependent\n",
    "H = nx.DiGraph()\n",
    "H.add_nodes_from(label_of.keys())\n",
    "\n",
    "# Extract all edge information first to help identify important relationships\n",
    "edge_counts = Counter()\n",
    "for _, r in edges.iterrows():\n",
    "    dep, pre = int(r[\":START_ID\"]), int(r[\":END_ID\"])  # CSV: dependent→prereq\n",
    "    edge_counts[(dep, pre)] += 1\n",
    "\n",
    "for _, r in edges.iterrows():\n",
    "    dep, pre = int(r[\":START_ID\"]), int(r[\":END_ID\"])  # CSV: dependent→prereq\n",
    "    labs_pre = label_of.get(pre, ())\n",
    "    labs_dep = label_of.get(dep, ())\n",
    "\n",
    "    # only keep if prereq is Concept or HardSkill, and dependent isn't Technology\n",
    "    if not (CONCEPT in labs_pre or HARDSKILL in labs_pre):\n",
    "        continue\n",
    "    if TECH in labs_dep:\n",
    "        continue\n",
    "\n",
    "    raw  = abs(float(r[\"score:float\"]))\n",
    "    \n",
    "    # Give small boost to frequently occurring edges - indicates importance\n",
    "    importance_factor = min(1.0 + 0.05 * edge_counts[(dep, pre)], 1.3)\n",
    "    raw = raw * importance_factor\n",
    "    \n",
    "    cost = raw + (PRED if r[\"predicted:boolean\"] else 0.0)\n",
    "\n",
    "    # Semantic similarity boost\n",
    "    if pre in emb and dep in emb:\n",
    "        sim = cosine(emb[pre], emb[dep])\n",
    "        if sim > 0.7:  # High similarity\n",
    "            cost = cost * (1.0 - SIM_BOOST)  # Lower cost for similar concepts\n",
    "\n",
    "    # avoid cycles\n",
    "    if nx.has_path(H, dep, pre):\n",
    "        continue\n",
    "\n",
    "    # insert or tighten existing edge\n",
    "    if H.has_edge(pre, dep):\n",
    "        H[pre][dep][\"cost\"] = min(H[pre][dep][\"cost\"], cost)\n",
    "        H[pre][dep][\"raw\"]  = min(H[pre][dep][\"raw\"],  raw)\n",
    "    else:\n",
    "        H.add_edge(pre, dep, cost=cost, raw=raw)\n",
    "\n",
    "assert nx.is_directed_acyclic_graph(H), \"H must be a DAG!\"\n",
    "\n",
    "# Find structural gaps in the knowledge graph\n",
    "def detect_and_fill_gaps():\n",
    "    \"\"\"\n",
    "    Detect structural gaps in the graph by finding skills that should be connected\n",
    "    based on semantic similarity but aren't linked in the graph.\n",
    "    \"\"\"\n",
    "    tech_nodes = [n for n in H.nodes() if TECH in label_of.get(n, set())]\n",
    "    skills_nodes = [n for n in H.nodes() if HARDSKILL in label_of.get(n, set())]\n",
    "    concept_nodes = [n for n in H.nodes() if CONCEPT in label_of.get(n, set())]\n",
    "    \n",
    "    additions = 0\n",
    "    \n",
    "    # Look for technologies that should connect to relevant skills\n",
    "    for tech in tech_nodes:\n",
    "        if tech not in emb:\n",
    "            continue\n",
    "            \n",
    "        # Find semantically similar skills or concepts\n",
    "        similar_nodes = []\n",
    "        for skill in skills_nodes + concept_nodes:\n",
    "            if skill not in emb:\n",
    "                continue\n",
    "                \n",
    "            sim = cosine(emb[tech], emb[skill])\n",
    "            if sim > 0.65:  # High similarity threshold\n",
    "                similar_nodes.append((skill, sim))\n",
    "        \n",
    "        # Sort by similarity and connect the top matches if not already connected\n",
    "        similar_nodes.sort(key=lambda x: x[1], reverse=True)\n",
    "        for skill, sim in similar_nodes[:3]:  # Top 3 most similar\n",
    "            if not H.has_edge(skill, tech) and not nx.has_path(H, tech, skill):\n",
    "                # Add edge from skill to tech (skill is prerequisite for tech)\n",
    "                H.add_edge(skill, tech, cost=0.6, raw=0.6)\n",
    "                additions += 1\n",
    "    \n",
    "    # Connect isolated skills to relevant concepts\n",
    "    orphan_skills = [n for n in H.nodes() \n",
    "                    if HARDSKILL in label_of.get(n, set()) \n",
    "                    and H.in_degree(n) == 0\n",
    "                    and H.out_degree(n) == 0]\n",
    "    \n",
    "    for orphan in orphan_skills:\n",
    "        if orphan not in emb:\n",
    "            continue\n",
    "            \n",
    "        # Find relevant concepts\n",
    "        similar_concepts = []\n",
    "        for concept in concept_nodes:\n",
    "            if concept not in emb:\n",
    "                continue\n",
    "                \n",
    "            sim = cosine(emb[orphan], emb[concept])\n",
    "            if sim > 0.6:\n",
    "                similar_concepts.append((concept, sim))\n",
    "        \n",
    "        # Connect to top matches\n",
    "        similar_concepts.sort(key=lambda x: x[1], reverse=True)\n",
    "        for concept, sim in similar_concepts[:2]:  # Top 2\n",
    "            # Add bidirectional edges to represent soft relationships\n",
    "            if not nx.has_path(H, orphan, concept) and not nx.has_path(H, concept, orphan):\n",
    "                H.add_edge(concept, orphan, cost=0.7, raw=0.7)\n",
    "                additions += 1\n",
    "    \n",
    "    print(f\"Added {additions} edges to fill structural gaps\")\n",
    "    return additions\n",
    "\n",
    "# Fill graph gaps\n",
    "detect_and_fill_gaps()\n",
    "\n",
    "# ─────────── USER INPUT ──────────────────────────────────\n",
    "# ---------------- USER INPUT -----------------\n",
    "user_skills = [\"graph theory\", \"computer vision\", \"neural network\", \"reinforcement learning\"]\n",
    "user_ids = { name2id[s.lower()] for s in user_skills if s.lower() in name2id }\n",
    "# ---------------------------------------------\n",
    "\n",
    "# helper: trim a path so it starts with the first node the user already knows\n",
    "def trim_to_user(path):\n",
    "    if path is None:\n",
    "        return None\n",
    "    for i, n in enumerate(path):\n",
    "        if n in user_ids:\n",
    "            return path[i:]\n",
    "    return path  # Return the full path if no user skill found\n",
    "\n",
    "# Common function to get required skills for the job\n",
    "def get_required_skills():\n",
    "    reqs = set(edges.loc[edges[\":START_ID\"] == job_id, \":END_ID\"].astype(int))\n",
    "    return reqs\n",
    "\n",
    "# Common function to categorize skills\n",
    "def create_skill_categories():\n",
    "    return {\n",
    "        \"already_known\": [],      # Skills the user already has\n",
    "        \"ready_to_learn\": [],     # Skills with direct paths from user knowledge\n",
    "        \"requires_prereqs\": [],   # Skills requiring longer paths\n",
    "        \"connected_orphans\": [],  # Orphans with alternative connections\n",
    "        \"true_orphans\": []        # Truly disconnected skills\n",
    "    }\n",
    "\n",
    "# Common function to print the results\n",
    "def print_skill_categories(skill_categories):\n",
    "    print(\"\\n=== SKILLS YOU ALREADY KNOW ===\")\n",
    "    for sid, _ in skill_categories[\"already_known\"]:\n",
    "        print(f\"• {id2name[sid]}\")\n",
    "\n",
    "    print(\"\\n=== SKILLS READY TO LEARN ===\")\n",
    "    for sid, path in skill_categories[\"ready_to_learn\"]:\n",
    "        print(f\"• {id2name[sid]:30}  depth={len(path)-1}\")\n",
    "        print(\"    \", \" → \".join(id2name[n] for n in path), \"\\n\")\n",
    "\n",
    "    print(\"\\n=== SKILLS REQUIRING PREREQUISITES ===\")\n",
    "    for sid, path in skill_categories[\"requires_prereqs\"]:\n",
    "        print(f\"• {id2name[sid]:30}  depth={len(path)-1}\")\n",
    "        print(\"    \", \" → \".join(id2name[n] for n in path), \"\\n\")\n",
    "\n",
    "    print(\"\\n=== ORPHAN SKILLS WITH CONNECTIONS TO YOUR KNOWLEDGE ===\")\n",
    "    for sid, path in skill_categories[\"connected_orphans\"]:\n",
    "        print(f\"• {id2name[sid]:30}  (semantic connection)\")\n",
    "        print(\"    \", \" → \".join(id2name[n] for n in path), \"\\n\")\n",
    "\n",
    "    print(\"\\n=== ORPHAN SKILLS ===\")\n",
    "    for sid, dependents in skill_categories[\"true_orphans\"]:\n",
    "        print(f\"✘ {id2name[sid]:30}  — no direct path from your skills\")\n",
    "        print(\"    Concepts that depend on this skill (learn next):\")\n",
    "        for d in dependents:\n",
    "            alt = build_chain_dp(d) or [d]\n",
    "            anchored = trim_to_user(alt) if user_ids else None\n",
    "            final_alt = anchored if anchored else alt\n",
    "            print(\"    •\", \" → \".join(id2name[n] for n in final_alt))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────── DYNAMIC PROGRAMMING CELL ──────────────────────────────\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Run the common code cell first before executing this cell\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 3) DP: longest + best-score path to every node with improved path quality\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dp \u001b[38;5;241m=\u001b[39m {n: \u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39minf \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[43mH\u001b[49m}\n\u001b[1;32m      6\u001b[0m prev \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      7\u001b[0m path_lens \u001b[38;5;241m=\u001b[39m {n: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m H}  \u001b[38;5;66;03m# Keep track of path lengths\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────── DYNAMIC PROGRAMMING CELL ──────────────────────────────\n",
    "# Run the common code cell first before executing this cell\n",
    "\n",
    "# 3) DP: longest + best-score path to every node with improved path quality\n",
    "dp = {n: -math.inf for n in H}\n",
    "prev = {}\n",
    "path_lens = {n: 0 for n in H}  # Keep track of path lengths\n",
    "\n",
    "# seed roots: any Concept/HardSkill with zero prerequisites\n",
    "for n in H:\n",
    "    if H.in_degree(n) == 0 and (CONCEPT in label_of[n] or HARDSKILL in label_of[n]):\n",
    "        dp[n] = 0.0\n",
    "        path_lens[n] = 1\n",
    "\n",
    "# Improved DP prioritizing path quality and reasonable length\n",
    "for u in nx.topological_sort(H):\n",
    "    if dp[u] < -1e8:\n",
    "        continue\n",
    "    for _, v, d in H.out_edges(u, data=True):\n",
    "        s_norm = d[\"raw\"] / Smax if Smax else 0.0\n",
    "        sem_pen = (1 - cosine(emb[u], emb[v])) if (u in emb and v in emb) else 1.0\n",
    "        \n",
    "        # Add length penalty to discourage excessively long paths\n",
    "        length_factor = LENGTH_PENALTY * path_lens[u]\n",
    "        \n",
    "        gain = Δ - α * s_norm - β * sem_pen - length_factor\n",
    "        cand = dp[u] + gain\n",
    "        \n",
    "        # Only consider if path length is reasonable\n",
    "        new_path_len = path_lens[u] + 1\n",
    "        if new_path_len <= MAX_PATH_LENGTH and cand > dp[v]:\n",
    "            dp[v], prev[v] = cand, u\n",
    "            path_lens[v] = new_path_len\n",
    "\n",
    "# back-track chain builder with length consideration\n",
    "def build_chain_dp(n):\n",
    "    if dp.get(n, -math.inf) < -1e8:\n",
    "        return None\n",
    "    path = [n]\n",
    "    while path[-1] in prev:\n",
    "        path.append(prev[path[-1]])\n",
    "        if len(path) > MAX_PATH_LENGTH:\n",
    "            break  # Enforce maximum path length\n",
    "    return list(reversed(path))\n",
    "\n",
    "# Function to find orphan skills' dependents (for skills without paths)\n",
    "def find_orphan_dependents(sid):\n",
    "    deps = edges.loc[edges[\":END_ID\"] == sid, \":START_ID\"].astype(int)\n",
    "    cands = [d for d in deps if (CONCEPT in label_of.get(d,()) or HARDSKILL in label_of.get(d,()))]\n",
    "    \n",
    "    scored = []\n",
    "    for d in cands:\n",
    "        r = edges[(edges[\":START_ID\"]==d)&(edges[\":END_ID\"]==sid)].iloc[0]\n",
    "        sc = abs(float(r[\"score:float\"])) + (PRED if r[\"predicted:boolean\"] else 0.0)\n",
    "        scored.append((d, sc))\n",
    "    scored.sort(key=lambda x: x[1])\n",
    "    \n",
    "    return [d for d, _ in scored[:SUGGEST_K]]\n",
    "\n",
    "# Run the DP-based analysis on required skills\n",
    "def run_dp_analysis():\n",
    "    print(f\"\\n{JOB.upper()} — DYNAMIC PROGRAMMING ANALYSIS\\n\")\n",
    "    \n",
    "    reqs = get_required_skills()\n",
    "    skill_categories = create_skill_categories()\n",
    "    \n",
    "    # Process each required skill\n",
    "    for sid in sorted(reqs, key=lambda x: id2name[x].lower()):\n",
    "        if not (label_of[sid] & END_LABELS):\n",
    "            continue  # ignore SoftSkills, Jobs, etc.\n",
    "        \n",
    "        # Check if already known\n",
    "        if sid in user_ids:\n",
    "            skill_categories[\"already_known\"].append((sid, [sid]))\n",
    "            continue\n",
    "            \n",
    "        # Try to find path via DP\n",
    "        dp_chain = build_chain_dp(sid)\n",
    "        dp_chain_anchored = trim_to_user(dp_chain) if dp_chain else None\n",
    "        \n",
    "        # If we have a good DP path anchored to user skills\n",
    "        if dp_chain_anchored and len(dp_chain_anchored) > 1 and dp_chain_anchored[0] in user_ids:\n",
    "            if len(dp_chain_anchored) <= 3:\n",
    "                skill_categories[\"ready_to_learn\"].append((sid, dp_chain_anchored))\n",
    "            else:\n",
    "                skill_categories[\"requires_prereqs\"].append((sid, dp_chain_anchored))\n",
    "            continue\n",
    "        \n",
    "        # If we have a DP path but not anchored to user skills\n",
    "        if dp_chain and len(dp_chain) > 1:\n",
    "            skill_categories[\"requires_prereqs\"].append((sid, dp_chain))\n",
    "            continue\n",
    "        \n",
    "        # Handle orphan skills - look for dependent skills\n",
    "        dependents = find_orphan_dependents(sid)\n",
    "        skill_categories[\"true_orphans\"].append((sid, dependents))\n",
    "    \n",
    "    # Print results\n",
    "    print_skill_categories(skill_categories)\n",
    "    \n",
    "    return skill_categories\n",
    "\n",
    "# Run the analysis\n",
    "dp_skill_categories = run_dp_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLP ENGINEER — A* SEARCH ANALYSIS\n",
      "\n",
      "\n",
      "=== SKILLS YOU ALREADY KNOW ===\n",
      "\n",
      "=== SKILLS READY TO LEARN ===\n",
      "• computer science                depth=1\n",
      "     graph theory → computer science \n",
      "\n",
      "• natural language processing     depth=1\n",
      "     graph theory → natural language processing \n",
      "\n",
      "• software engineering            depth=2\n",
      "     graph theory → computer science → software engineering \n",
      "\n",
      "\n",
      "=== SKILLS REQUIRING PREREQUISITES ===\n",
      "• scalability                     depth=4\n",
      "     graph theory → computer science → programming language → software design → scalability \n",
      "\n",
      "\n",
      "=== ORPHAN SKILLS WITH CONNECTIONS TO YOUR KNOWLEDGE ===\n",
      "\n",
      "=== ORPHAN SKILLS ===\n",
      "✘ algorithms                      — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • foundations of mathematics → mathematics → probability → context sensitive grammar → anaphora resolution → discourse model\n",
      "    • cuda → message passing interface\n",
      "    • algorithms → address locator\n",
      "    • genai\n",
      "    • graph theory → computer science → algorithm → distributed algorithms\n",
      "\n",
      "✘ bit error rate tester (bert)    — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ deep learning                   — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • llms\n",
      "\n",
      "✘ elasticsearch                   — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ language model                  — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • foundations of mathematics → mathematics → polynomial → linear algebra → activation functions → backpropagation through time → long shortterm memory networks\n",
      "    • foundations of mathematics → mathematics → polynomial → linear algebra → activation functions → backpropagation through time → generative adversarial networks\n",
      "    • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → character level language models → transliteration\n",
      "    • foundations of mathematics → mathematics → polynomial → linear algebra → differential calculus → gradient descent → stack lstm\n",
      "    • elasticsearch\n",
      "\n",
      "✘ library                         — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ machine learning methods        — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • machine learning methods → ensemble classifier\n",
      "    • foundations of mathematics → mathematics → polynomial → linear algebra → activation functions → backpropagation through time → generative adversarial networks\n",
      "    • convolutional neural networks → facial recognition\n",
      "    • genai\n",
      "    • llms\n",
      "\n",
      "✘ named entity recognition        — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → relation extraction\n",
      "    • foundations of mathematics → mathematics → probability → conditional probability → bayes theorem → event detection\n",
      "\n",
      "✘ numpy                           — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • dask\n",
      "    • llms\n",
      "\n",
      "✘ pandas                          — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ python                          — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ pytorch                         — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ scikit-learn                    — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • foundations of mathematics → mathematics → polynomial → linear algebra → activation functions → backpropagation through time → generative adversarial networks\n",
      "    • rllib\n",
      "    • rpa\n",
      "    • graph theory → computer science → p versus np problem\n",
      "\n",
      "✘ scipy                           — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ spacy                           — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ tensorflow                      — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • fairseq\n",
      "    • llms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────── A* SEARCH CELL ──────────────────────────────\n",
    "# Run the common code cell first before executing this cell\n",
    "\n",
    "# A* search implementation for finding paths\n",
    "def find_astar_path(start, target, max_length=MAX_PATH_LENGTH):\n",
    "    \"\"\"Find path using A* search with embedding similarity as heuristic.\"\"\"\n",
    "    if start not in H or target not in H:\n",
    "        return None\n",
    "        \n",
    "    # A* search with heuristic based on embedding similarity\n",
    "    def heuristic(n):\n",
    "        if n in emb and target in emb:\n",
    "            return 1 - cosine(emb[n], emb[target])  # Lower for similar nodes\n",
    "        return 1.0\n",
    "    \n",
    "    open_set = [(0, 0, start, [start])]  # (f_score, g_score, node, path)\n",
    "    closed_set = set()\n",
    "    \n",
    "    while open_set:\n",
    "        _, g_score, current, path = heapq.heappop(open_set)\n",
    "        \n",
    "        if current == target:\n",
    "            return path\n",
    "            \n",
    "        if current in closed_set or len(path) > max_length:\n",
    "            continue\n",
    "            \n",
    "        closed_set.add(current)\n",
    "        \n",
    "        for _, neighbor in H.out_edges(current):\n",
    "            if neighbor in closed_set:\n",
    "                continue\n",
    "                \n",
    "            tentative_g = g_score + 1\n",
    "            f_score = tentative_g + heuristic(neighbor)\n",
    "            \n",
    "            heapq.heappush(open_set, (f_score, tentative_g, neighbor, path + [neighbor]))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Find paths from any user skill to a target skill\n",
    "def find_paths_from_user_skills(target_id, max_length=MAX_PATH_LENGTH):\n",
    "    \"\"\"Find paths from any user skill to the target skill.\"\"\"\n",
    "    if not user_ids:\n",
    "        return None\n",
    "    \n",
    "    best_path = None\n",
    "    best_score = float('inf')  # Lower is better for A*\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        path = find_astar_path(user_id, target_id, max_length)\n",
    "        if path and (best_path is None or len(path) < best_score):\n",
    "            best_path = path\n",
    "            best_score = len(path)\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "# Find semantic connections between skills\n",
    "def find_semantic_connection(target_id, threshold=0.5):\n",
    "    \"\"\"Find semantically similar user skills.\"\"\"\n",
    "    if not user_ids or target_id not in emb:\n",
    "        return None\n",
    "    \n",
    "    best_match = None\n",
    "    best_sim = -1\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        if user_id not in emb:\n",
    "            continue\n",
    "        \n",
    "        sim = cosine(emb[user_id], emb[target_id])\n",
    "        if sim > threshold and sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_match = user_id\n",
    "    \n",
    "    if best_match:\n",
    "        return [best_match, target_id]\n",
    "    return None\n",
    "\n",
    "# Function to find orphan skills' dependents using A*\n",
    "def find_orphan_paths_astar(sid):\n",
    "    deps = edges.loc[edges[\":END_ID\"] == sid, \":START_ID\"].astype(int)\n",
    "    cands = [d for d in deps if (CONCEPT in label_of.get(d,()) or HARDSKILL in label_of.get(d,()))]\n",
    "    \n",
    "    paths = []\n",
    "    for d in cands[:SUGGEST_K]:  # Only process top K dependents\n",
    "        # Try to find paths from user skills to this dependent\n",
    "        best_path = None\n",
    "        for user_id in user_ids:\n",
    "            path = find_astar_path(user_id, d, max_length=3)\n",
    "            if path:\n",
    "                if not best_path or len(path) < len(best_path):\n",
    "                    best_path = path\n",
    "        \n",
    "        if best_path:\n",
    "            paths.append((d, best_path))\n",
    "        else:\n",
    "            paths.append((d, [d]))  # Just the node itself\n",
    "    \n",
    "    # Sort paths by length (shorter is better)\n",
    "    paths.sort(key=lambda x: len(x[1]))\n",
    "    return [d for d, _ in paths[:SUGGEST_K]]\n",
    "\n",
    "# Run the A* analysis on required skills\n",
    "def run_astar_analysis():\n",
    "    print(f\"\\n{JOB.upper()} — A* SEARCH ANALYSIS\\n\")\n",
    "    \n",
    "    reqs = get_required_skills()\n",
    "    skill_categories = create_skill_categories()\n",
    "    \n",
    "    # Process each required skill\n",
    "    for sid in sorted(reqs, key=lambda x: id2name[x].lower()):\n",
    "        if not (label_of[sid] & END_LABELS):\n",
    "            continue  # ignore SoftSkills, Jobs, etc.\n",
    "        \n",
    "        # Check if already known\n",
    "        if sid in user_ids:\n",
    "            skill_categories[\"already_known\"].append((sid, [sid]))\n",
    "            continue\n",
    "            \n",
    "        # Try to find direct A* path from user skills\n",
    "        astar_path = find_paths_from_user_skills(sid, max_length=MAX_PATH_LENGTH)\n",
    "        \n",
    "        if astar_path and len(astar_path) > 1:\n",
    "            if len(astar_path) <= 3:\n",
    "                skill_categories[\"ready_to_learn\"].append((sid, astar_path))\n",
    "            else:\n",
    "                skill_categories[\"requires_prereqs\"].append((sid, astar_path))\n",
    "            continue\n",
    "        \n",
    "        # Try semantic connections for orphans\n",
    "        semantic_path = find_semantic_connection(sid, threshold=0.45)\n",
    "        if semantic_path:\n",
    "            skill_categories[\"connected_orphans\"].append((sid, semantic_path))\n",
    "            continue\n",
    "        \n",
    "        # Handle true orphan skills\n",
    "        dependents = find_orphan_paths_astar(sid)\n",
    "        skill_categories[\"true_orphans\"].append((sid, dependents))\n",
    "    \n",
    "    # Print results\n",
    "    print_skill_categories(skill_categories)\n",
    "    \n",
    "    return skill_categories\n",
    "\n",
    "# Run the analysis\n",
    "astar_skill_categories = run_astar_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMPUTER VISION ENGINEER — COMBINED ANALYSIS (DP + A*)\n",
      "\n",
      "\n",
      "=== SKILLS YOU ALREADY KNOW ===\n",
      "• computer vision\n",
      "\n",
      "=== SKILLS READY TO LEARN ===\n",
      "• computer science                depth=1\n",
      "     graph theory → computer science \n",
      "\n",
      "• machine learning                depth=1\n",
      "     graph theory → machine learning \n",
      "\n",
      "• object detection                depth=1\n",
      "     computer vision → object detection \n",
      "\n",
      "• robotics                        depth=2\n",
      "     graph theory → artificial intelligence → robotics \n",
      "\n",
      "\n",
      "=== SKILLS REQUIRING PREREQUISITES ===\n",
      "• image processing                depth=3\n",
      "     graph theory → artificial intelligence → domain adaptation → image processing \n",
      "\n",
      "\n",
      "=== ORPHAN SKILLS WITH CONNECTIONS TO YOUR KNOWLEDGE ===\n",
      "\n",
      "=== ORPHAN SKILLS ===\n",
      "✘ algorithms                      — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • genai\n",
      "    • cuda → message passing interface\n",
      "    • foundations of mathematics → mathematics → polynomial → algorithm → distributed algorithms\n",
      "    • llms\n",
      "    • algorithms → address locator\n",
      "\n",
      "✘ aws                             — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • sagemaker\n",
      "    • cuda → message passing interface\n",
      "    • amazon lex\n",
      "\n",
      "✘ c++                             — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • foundations of mathematics → mathematics → polynomial → algorithm → p versus np problem\n",
      "    • foundations of mathematics → mathematics → polynomial → algorithm → genetic algorithm\n",
      "    • cuda → message passing interface\n",
      "    • foundations of mathematics → mathematics → polynomial → algorithm → algorithm design → database → database design\n",
      "    • foundations of mathematics → mathematics → polynomial → algorithm → computer programming → programmer\n",
      "\n",
      "✘ deep learning                   — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • llms\n",
      "\n",
      "✘ docker                          — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ keras                           — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ kubernetes                      — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • cuda → message passing interface\n",
      "    • elasticsearch\n",
      "    • api gateway\n",
      "    • genai\n",
      "\n",
      "✘ machine learning methods        — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • foundations of mathematics → mathematics → probability → descriptive statistics and hypothesis testing → generative adversarial networks\n",
      "    • llms\n",
      "    • machine learning methods → ensemble classifier\n",
      "    • genai\n",
      "    • rllib\n",
      "\n",
      "✘ opencv                          — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ python                          — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ pytorch                         — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "\n",
      "✘ sql                             — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • foundations of mathematics → mathematics → polynomial → algorithm → algorithm design → database → query optimization\n",
      "\n",
      "✘ tensorflow                      — no direct path from your skills\n",
      "    Concepts that depend on this skill (learn next):\n",
      "    • llms\n",
      "    • fairseq\n",
      "\n",
      "\n",
      "=== COMPARISON OF APPROACHES ===\n",
      "Dynamic Programming:\n",
      "{'already_known': 1, 'ready_to_learn': 0, 'requires_prereqs': 5, 'connected_orphans': 0, 'true_orphans': 13}\n",
      "\n",
      "A* Search:\n",
      "{'already_known': 1, 'ready_to_learn': 4, 'requires_prereqs': 1, 'connected_orphans': 0, 'true_orphans': 13}\n",
      "\n",
      "Combined Approach:\n",
      "{'already_known': 1, 'ready_to_learn': 4, 'requires_prereqs': 1, 'connected_orphans': 0, 'true_orphans': 13}\n",
      "\n",
      "=== AVERAGE PATH LENGTHS ===\n",
      "Dynamic Programming: 5.40\n",
      "A* Search: 2.60\n",
      "Combined Approach: 2.60\n",
      "\n",
      "=== OVERALL EFFECTIVENESS (% of skills with valid paths) ===\n",
      "Dynamic Programming: 31.58%\n",
      "A* Search: 31.58%\n",
      "Combined Approach: 31.58%\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────── COMBINED APPROACH CELL ──────────────────────────────\n",
    "# Run the common code cell first, then run both DP and A* cells before executing this cell\n",
    "\n",
    "# Combined path-finding approach that leverages both DP and A*\n",
    "def find_best_path(sid):\n",
    "    \"\"\"Find the best path to a skill using both DP and A*.\"\"\"\n",
    "    best_path = None\n",
    "    best_score = float('inf')  # Lower is better\n",
    "    \n",
    "    # 1. Try DP path\n",
    "    dp_path = build_chain_dp(sid)\n",
    "    dp_path_anchored = trim_to_user(dp_path) if dp_path else None\n",
    "    \n",
    "    if dp_path_anchored and dp_path_anchored[0] in user_ids:\n",
    "        best_path = dp_path_anchored\n",
    "        best_score = len(dp_path_anchored)\n",
    "    elif dp_path:\n",
    "        best_path = dp_path\n",
    "        best_score = len(dp_path) + 10  # Penalty for not being anchored\n",
    "    \n",
    "    # 2. Try A* paths from each user skill\n",
    "    for user_id in user_ids:\n",
    "        astar_path = find_astar_path(user_id, sid, max_length=MAX_PATH_LENGTH)\n",
    "        if astar_path and len(astar_path) < best_score:\n",
    "            best_path = astar_path\n",
    "            best_score = len(astar_path)\n",
    "    \n",
    "    # 3. If we still don't have a path, try connecting through intermediates\n",
    "    if best_path is None and dp_path:\n",
    "        # Try to connect user skills to the start of the DP path\n",
    "        for user_id in user_ids:\n",
    "            bridge_path = find_astar_path(user_id, dp_path[0], max_length=2)\n",
    "            if bridge_path:\n",
    "                # Combine paths without duplicating the connecting node\n",
    "                combined_path = bridge_path[:-1] + dp_path\n",
    "                if best_path is None or len(combined_path) < best_score:\n",
    "                    best_path = combined_path\n",
    "                    best_score = len(combined_path)\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "# Find semantic connections for true orphans\n",
    "def find_best_semantic_connection(sid, threshold=0.4):\n",
    "    \"\"\"Find the best semantic connection to user skills.\"\"\"\n",
    "    if sid not in emb:\n",
    "        return None\n",
    "    \n",
    "    # Direct semantic similarity\n",
    "    best_direct = find_semantic_connection(sid, threshold)\n",
    "    \n",
    "    # Two-hop semantic bridges\n",
    "    best_bridge = None\n",
    "    best_bridge_score = -1\n",
    "    \n",
    "    # Find intermediates that connect user skills to the target\n",
    "    for n in H.nodes():\n",
    "        if n not in emb or n == sid:\n",
    "            continue\n",
    "        \n",
    "        # Check if this node connects well to both user skills and target\n",
    "        target_sim = cosine(emb[n], emb[sid])\n",
    "        if target_sim < 0.5:  # Minimum similarity to target\n",
    "            continue\n",
    "        \n",
    "        # Check similarity to user skills\n",
    "        for user_id in user_ids:\n",
    "            if user_id not in emb:\n",
    "                continue\n",
    "            \n",
    "            user_sim = cosine(emb[user_id], emb[n])\n",
    "            if user_sim > 0.5:  # Good similarity to user skill\n",
    "                bridge_score = (user_sim + target_sim) / 2\n",
    "                if bridge_score > best_bridge_score:\n",
    "                    best_bridge_score = bridge_score\n",
    "                    best_bridge = [user_id, n, sid]\n",
    "    \n",
    "    # Return the best option\n",
    "    if best_bridge and best_bridge_score > 0.6:\n",
    "        return best_bridge\n",
    "    return best_direct\n",
    "\n",
    "# Improved orphan handling\n",
    "def handle_orphan_skill(sid):\n",
    "    \"\"\"Comprehensive approach for orphan skills.\"\"\"\n",
    "    # 1. Try semantic connections\n",
    "    semantic_path = find_best_semantic_connection(sid)\n",
    "    if semantic_path:\n",
    "        return \"connected_orphans\", semantic_path\n",
    "    \n",
    "    # 2. Find dependents\n",
    "    deps = edges.loc[edges[\":END_ID\"] == sid, \":START_ID\"].astype(int)\n",
    "    cands = [d for d in deps if (CONCEPT in label_of.get(d,()) or HARDSKILL in label_of.get(d,()))]\n",
    "    \n",
    "    # 3. Score dependents by multiple factors\n",
    "    scored_deps = []\n",
    "    for d in cands:\n",
    "        # Get edge score\n",
    "        r = edges[(edges[\":START_ID\"]==d)&(edges[\":END_ID\"]==sid)].iloc[0]\n",
    "        raw_score = abs(float(r[\"score:float\"])) + (PRED if r[\"predicted:boolean\"] else 0.0)\n",
    "        \n",
    "        # Check if this dependent has a path from user skills\n",
    "        path_quality = 0\n",
    "        best_path = find_best_path(d)\n",
    "        if best_path and best_path[0] in user_ids:\n",
    "            path_quality = 1.0 / len(best_path)  # Shorter paths are better\n",
    "        \n",
    "        # Combine factors\n",
    "        final_score = raw_score * (1 - 0.3 * path_quality)  # Lower is better\n",
    "        scored_deps.append((d, final_score, best_path))\n",
    "    \n",
    "    # Sort by final score\n",
    "    scored_deps.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Return top dependents\n",
    "    return \"true_orphans\", [d for d, _, _ in scored_deps[:SUGGEST_K]]\n",
    "\n",
    "# Run the combined analysis on required skills\n",
    "def run_combined_analysis():\n",
    "    print(f\"\\n{JOB.upper()} — COMBINED ANALYSIS (DP + A*)\\n\")\n",
    "    \n",
    "    reqs = get_required_skills()\n",
    "    skill_categories = create_skill_categories()\n",
    "    \n",
    "    # Process each required skill\n",
    "    for sid in sorted(reqs, key=lambda x: id2name[x].lower()):\n",
    "        if not (label_of[sid] & END_LABELS):\n",
    "            continue  # ignore SoftSkills, Jobs, etc.\n",
    "        \n",
    "        # Check if already known\n",
    "        if sid in user_ids:\n",
    "            skill_categories[\"already_known\"].append((sid, [sid]))\n",
    "            continue\n",
    "            \n",
    "        # Try to find the best path using both DP and A*\n",
    "        best_path = find_best_path(sid)\n",
    "        \n",
    "        if best_path and len(best_path) > 1:\n",
    "            # Check if anchored to user skills\n",
    "            if best_path[0] in user_ids:\n",
    "                if len(best_path) <= 3:\n",
    "                    skill_categories[\"ready_to_learn\"].append((sid, best_path))\n",
    "                else:\n",
    "                    skill_categories[\"requires_prereqs\"].append((sid, best_path))\n",
    "            else:\n",
    "                skill_categories[\"requires_prereqs\"].append((sid, best_path))\n",
    "            continue\n",
    "        \n",
    "        # Handle orphan skills\n",
    "        category, result = handle_orphan_skill(sid)\n",
    "        skill_categories[category].append((sid, result))\n",
    "    \n",
    "    # Print results\n",
    "    print_skill_categories(skill_categories)\n",
    "    \n",
    "    return skill_categories\n",
    "\n",
    "# Run the combined analysis\n",
    "combined_skill_categories = run_combined_analysis()\n",
    "\n",
    "# Compare results from different approaches\n",
    "def count_by_category(categories):\n",
    "    return {k: len(v) for k, v in categories.items()}\n",
    "\n",
    "print(\"\\n=== COMPARISON OF APPROACHES ===\")\n",
    "print(\"Dynamic Programming:\")\n",
    "dp_counts = count_by_category(dp_skill_categories)\n",
    "print(dp_counts)\n",
    "\n",
    "print(\"\\nA* Search:\")\n",
    "astar_counts = count_by_category(astar_skill_categories)\n",
    "print(astar_counts)\n",
    "\n",
    "print(\"\\nCombined Approach:\")\n",
    "combined_counts = count_by_category(combined_skill_categories)\n",
    "print(combined_counts)\n",
    "\n",
    "# Calculate path lengths for comparison\n",
    "def calculate_average_path_length(categories):\n",
    "    path_lengths = []\n",
    "    for category in [\"ready_to_learn\", \"requires_prereqs\", \"connected_orphans\"]:\n",
    "        for _, path in categories[category]:\n",
    "            if isinstance(path, list) and len(path) > 1:\n",
    "                path_lengths.append(len(path))\n",
    "    \n",
    "    if not path_lengths:\n",
    "        return 0\n",
    "    return sum(path_lengths) / len(path_lengths)\n",
    "\n",
    "print(\"\\n=== AVERAGE PATH LENGTHS ===\")\n",
    "print(f\"Dynamic Programming: {calculate_average_path_length(dp_skill_categories):.2f}\")\n",
    "print(f\"A* Search: {calculate_average_path_length(astar_skill_categories):.2f}\")\n",
    "print(f\"Combined Approach: {calculate_average_path_length(combined_skill_categories):.2f}\")\n",
    "\n",
    "# Calculate overall effectiveness (% of skills with valid paths)\n",
    "def calculate_effectiveness(categories):\n",
    "    total_skills = sum(len(v) for v in categories.values())\n",
    "    orphan_count = len(categories[\"true_orphans\"])\n",
    "    \n",
    "    if total_skills == 0:\n",
    "        return 0\n",
    "    return 100 * (total_skills - orphan_count) / total_skills\n",
    "\n",
    "print(\"\\n=== OVERALL EFFECTIVENESS (% of skills with valid paths) ===\")\n",
    "print(f\"Dynamic Programming: {calculate_effectiveness(dp_skill_categories):.2f}%\")\n",
    "print(f\"A* Search: {calculate_effectiveness(astar_skill_categories):.2f}%\")\n",
    "print(f\"Combined Approach: {calculate_effectiveness(combined_skill_categories):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
